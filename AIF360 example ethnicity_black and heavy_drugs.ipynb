{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Bias Assessment & Mitigation Techniques in Machine Learning\n",
    "## Applying the AIF360 & DALEX Toolkit on the Drug Consumption Dataset\n",
    "\n",
    "Data Science & Artifial Intelligence Course Project\n",
    "\n",
    "by Linda Gahleitner & Felix Mühle\n",
    "\n",
    "**Project context and objective**\n",
    "\n",
    "The project aims to develop a predictive machine learning model using the AIF360 and DALEX toolkits to analyze the drug consumption dataset from the UCI Irvine Machine Learning Repository. The model will classify individuals into active and non-active drug users based on their reported usage of various substances. The primary target variable will categorize substances into four distinct groups: legal light drugs, party drugs, psychedelic drugs, and heavy drugs. Protected attributes such as gender, ethnicity, and personality scores will be examined to ensure the model's fairness and bias mitigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use-Case\n",
    "\n",
    "In a hypothetical scenario, the prediction model could be employed by law enforcement agencies to identify individuals at risk of active drug use. This could be used for:\n",
    "\n",
    "- Preventive Measures: Implementing targeted interventions and preventive measures for individuals at higher risk of substance abuse.\n",
    "- Resource Allocation: Efficiently allocating resources and support services to areas with a higher predicted prevalence of drug use.\n",
    "- Policy Making: Informing policy decisions regarding drug prevention programs and law enforcement strategies.\n",
    "\n",
    "**Importance of Fair and Bias-Free Machine Learning Model**\n",
    "\n",
    "- Non-Discrimination: Ensuring that the model does not discriminate against individuals based on gender, ethnicity, or personality scores is critical to uphold ethical standards. Discriminatory practices can lead to unfair treatment and reinforce societal biases.\n",
    "- Privacy and Consent: Respecting individuals' privacy and ensuring informed consent for the use of their data is fundamental. The misuse of predictive models can lead to ethical breaches and loss of public trust.\n",
    "- Regulatory Requirements: Many jurisdictions have regulations and laws (e.g., GDPR, CCPA) that mandate the fair use of data and prevent discrimination. Ensuring compliance with these regulations is essential to avoid legal repercussions.\n",
    "- Civil Rights Protection: Protecting the civil rights of individuals by preventing bias in predictive models is crucial to avoid potential lawsuits and public backlash.\n",
    "- Public Trust: A fair and unbiased model fosters trust in law enforcement agencies and the broader use of machine learning in public services. Erosion of trust can lead to resistance against beneficial technological advancements.\n",
    "- Social Equity: Promoting social equity by ensuring that predictive models do not disproportionately harm marginalized or vulnerable groups. Bias in models can exacerbate existing social inequalities and lead to unjust outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data used\n",
    "\n",
    "[The Drug Consumption (Quantified) dataset from UCI](https://archive.ics.uci.edu/dataset/373/drug+consumption+quantified) contains records for 1885 respondents, each with 12 attributes. These attributes include demographic information (age, gender, education, country, ethnicity), personality measurements (neuroticism, extraversion, openness, agreeableness, conscientiousness, impulsiveness, sensation seeking), and responses to questions about the use of 18 substances. Usage is categorized into seven classes: \"Never Used\", \"Used over a Decade Ago\", \"Used in Last Decade\", \"Used in Last Year\", \"Used in Last Month\", \"Used in Last Week\", and \"Used in Last Day\".\n",
    "\n",
    "**Feature Creation and Adaptations**\n",
    "\n",
    "*Drug Usage Classification:*\n",
    "- Legal Light Drugs: Alcohol, Nicotine\n",
    "- Party Drugs: Cocaine, Ecstasy, Amphetamines\n",
    "- Psychedelic Drugs: LSD, Mushrooms\n",
    "- Heavy Drugs: Methadone, Heroin, Crack\n",
    "\n",
    "Respondents who used any substance in the respective category within the last year are marked as \"1\" (active user), otherwise \"0\" (non-user).\n",
    "\n",
    "*Protected Attributes:*\n",
    "- Gender\n",
    "- Ethnicity\n",
    "- Personality Scores (NEO-FFI-R measurements: Neuroticism, Extraversion, Openness to Experience, Agreeableness, Conscientiousness)\n",
    "\n",
    "*Target Variable Transformation:*\n",
    "\n",
    "Consolidated drug usage responses into binary classification: \"Never used\", \"Used over a decade ago\", and \"Used in last decade\" are labeled as non-users (0), while \"Used in last year\", \"Used in last month\", \"Used in last week\", and \"Used in last day\" are labeled as active users (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Important Concepts\n",
    "\n",
    "### 3.1 Boundary between protected and unprotected attributes\n",
    "\n",
    "**Protected Attributes:**\n",
    "\n",
    "Characteristics explicitly safeguarded against discrimination by laws and regulations. Under GDPR, these include gender, race, ethnicity, religion, sexual orientation, and more.\n",
    "\n",
    "**Unprotected Attributes:**\n",
    "\n",
    "Features not explicitly covered by anti-discrimination laws, such as certain personality scores, unless they indirectly cause discrimination based on protected characteristics.\n",
    "\n",
    "Gender and race are protected attributes under GDPR, ensuring that personal data processing does not lead to discrimination. Personality scores are not explicitly protected but could be relevant if they cause indirect discrimination.\n",
    "\n",
    "### 3.2 Understanding of the metrics\n",
    "\n",
    "**Statistical Parity Difference:**\n",
    "- Measures the difference in positive outcomes between groups. \n",
    "- Example: Ensuring hiring rates are equal across genders.\n",
    "- Desirable Value: Close to 0. A value of 0 indicates no disparity between the groups. Positive or negative values indicate bias.\n",
    "\n",
    "**Disparate Impact:**\n",
    "- Compares the rate of favorable outcomes for protected vs. unprotected groups. \n",
    "- Example: Ensuring loan approval rates for minorities are not disproportionately lower.\n",
    "- Desirable Value: Close to 1. A value of 1 indicates no disparity between groups. Values less than 1 or greater than 1 indicate bias.\n",
    "\n",
    "**Consistency:** \n",
    "- Assesses if similar individuals receive similar predictions. \n",
    "- Example: Ensuring students with similar grades get similar college admission decisions.\n",
    "- Desirable Value: Close to 1. A value of 1 indicates that similar individuals receive similar predictions. Lower values indicate inconsistency in the predictions.\n",
    "\n",
    "**Equal Opportunity Difference:** \n",
    "- Difference in true positive rates across groups. \n",
    "- Example: Ensuring equal cancer detection rates across races.\n",
    "- Desirable Value: Close to 0. A value of 0 indicates equal true positive rates for both groups. Positive or negative values indicate bias.\n",
    "\n",
    "**Average Odds Difference:** \n",
    "- Average of differences in false positive and true positive rates. \n",
    "- Example: Balancing error rates in criminal recidivism predictions across races.\n",
    "- Desirable Value: Close to 0. A value of 0 indicates equal false positive rates and true positive rates for both groups. Positive or negative values indicate bias.\n",
    "\n",
    "**Theil Index:**\n",
    "- Measures inequality in predictions. \n",
    "- Example: Income prediction fairness across socioeconomic groups.\n",
    "- Desirable Value: Close to 0. A value of 0 indicates perfect fairness. Higher values indicate more inequality.\n",
    "\n",
    "**Error Rate Difference:**\n",
    "- Difference in overall error rates between groups. \n",
    "- Example: Ensuring equal error rates in medical diagnosis predictions for different genders.\n",
    "- Desirable Value: Close to 0. A value of 0 indicates equal error rates for both groups. Positive or negative values indicate bias.\n",
    "\n",
    "**False Negative Rate Difference:**\n",
    "- Difference in false negatives across groups. \n",
    "- Example: Ensuring no group is disproportionately denied job opportunities.\n",
    "- Desirable Value: Close to 0. A value of 0 indicates equal false negative rates for both groups. Positive or negative values indicate bias.\n",
    "\n",
    "**False Positive Rate Difference:**\n",
    "- Difference in false positives across groups. \n",
    "- Example: Ensuring no group is disproportionately flagged for fraud.\n",
    "- Desirable Value: Close to 0. A value of 0 indicates equal false positive rates for both groups. Positive or negative values indicate bias.\n",
    "\n",
    "**Binary Confusion Matrix:**\n",
    "- Table showing true vs. predicted classifications. \n",
    "- Example: Evaluating a model’s performance in predicting loan defaults, showing true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "**Classification Accuracy:**\n",
    "- Overall correctness of predictions. \n",
    "- Example: Accuracy of a spam email filter.\n",
    "\n",
    "### 3.3 Important metrics for our project\n",
    "\n",
    "**High Importance**\n",
    "- Equal Opportunity Difference: Ensuring fair detection rates of active drug users across all protected attributes is crucial for preventing discriminatory practices.\n",
    "- Disparate Impact: Preventing disproportionately lower favorable outcomes (non-drug users) for protected groups is essential for fairness and legal compliance.\n",
    "- False Positive Rate Difference: Minimizing incorrect identification of active drug users across groups is critical to avoid unjust treatment and stigmatization.\n",
    "\n",
    "**Medium Importance**\n",
    "- Statistical Parity Difference: Important for ensuring similar rates of active drug users across groups, although it may not capture all nuances of fairness.\n",
    "- Consistency: Ensures individuals with similar attributes receive fair predictions, but may be secondary to outcome-focused metrics.\n",
    "- Theil Index: Measures overall inequality, important but less direct than specific error rate metrics.\n",
    "\n",
    "**Low Importance**\n",
    "- Classification Accuracy: Overall correctness is important but less critical than ensuring specific fairness metrics.\n",
    "- Error Rate Difference: Ensures equal error rates, but detailed error type metrics (false positives/negatives) are more actionable.\n",
    "- False Negative Rate Difference: Important to minimize missed active users but secondary to preventing false positives.\n",
    "- Binary Confusion Matrix: Provides comprehensive performance overview but less focused on fairness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fetching and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ucimlrepo\n",
      "  Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
      "Collecting certifi>=2020.12.5\n",
      "  Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ucimlrepo) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.0.0->ucimlrepo) (1.19.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.0.0->ucimlrepo) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.0.0->ucimlrepo) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->ucimlrepo) (1.15.0)\n",
      "Installing collected packages: certifi, ucimlrepo\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2020.6.20\n",
      "    Uninstalling certifi-2020.6.20:\n",
      "      Successfully uninstalled certifi-2020.6.20\n",
      "Successfully installed certifi-2024.7.4 ucimlrepo-0.0.7\n",
      "Collecting aif360\n",
      "  Downloading aif360-0.6.1-py3-none-any.whl (259 kB)\n",
      "\u001b[K     |████████████████████████████████| 259 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from aif360) (3.3.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.8/site-packages (from aif360) (1.1.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from aif360) (1.5.2)\n",
      "Collecting scikit-learn>=1.0\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 50.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.8/site-packages (from aif360) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->aif360) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/conda/lib/python3.8/site-packages (from matplotlib->aif360) (2024.7.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib->aif360) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->aif360) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->aif360) (7.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->aif360) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24.0->aif360) (2020.1)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 46.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=1.0->aif360) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib->aif360) (1.15.0)\n",
      "Installing collected packages: joblib, scikit-learn, aif360\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 0.17.0\n",
      "    Uninstalling joblib-0.17.0:\n",
      "      Successfully uninstalled joblib-0.17.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.2\n",
      "    Uninstalling scikit-learn-0.23.2:\n",
      "      Successfully uninstalled scikit-learn-0.23.2\n",
      "Successfully installed aif360-0.6.1 joblib-1.4.2 scikit-learn-1.3.2\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries...\n",
    "!pip install ucimlrepo\n",
    "!pip install aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Fetching the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 373, 'name': 'Drug Consumption (Quantified)', 'repository_url': 'https://archive.ics.uci.edu/dataset/373/drug+consumption+quantified', 'data_url': 'https://archive.ics.uci.edu/static/public/373/data.csv', 'abstract': 'Classify type of drug consumer by personality data', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 1885, 'num_features': 12, 'feature_types': ['Real'], 'demographics': ['Age', 'Gender', 'Education Level', 'Nationality', 'Ethnicity'], 'target_col': ['alcohol', 'amphet', 'amyl', 'benzos', 'caff', 'cannabis', 'choc', 'coke', 'crack', 'ecstasy', 'heroin', 'ketamine', 'legalh', 'lsd', 'meth', 'mushrooms', 'nicotine', 'semer', 'vsa'], 'index_col': ['id'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2015, 'last_updated': 'Fri Mar 08 2024', 'dataset_doi': '10.24432/C5TC7S', 'creators': ['Elaine Fehrman', 'Vincent Egan', 'Evgeny Mirkes'], 'intro_paper': {'title': 'The Five Factor Model of personality and evaluation of drug consumption risk', 'authors': 'E. Fehrman, A. Muhammad, E. Mirkes, Vincent Egan, A. Gorban', 'published_in': 'Data Science', 'year': 2015, 'url': 'https://arxiv.org/abs/1506.06297', 'doi': None}, 'additional_info': {'summary': 'Database contains records for 1885 respondents. For each respondent 12 attributes are known: Personality measurements which include NEO-FFI-R (neuroticism, extraversion, openness to experience, agreeableness, and conscientiousness), BIS-11 (impulsivity), and ImpSS (sensation seeking), level of education, age, gender, country of residence and ethnicity. All input attributes are originally categorical and are quantified. After quantification values of all input features can be considered as real-valued. In addition, participants were questioned concerning their use of 18 legal and illegal drugs (alcohol, amphetamines, amyl nitrite, benzodiazepine, cannabis, chocolate, cocaine, caffeine, crack, ecstasy, heroin, ketamine, legal highs, LSD, methadone, mushrooms, nicotine and volatile substance abuse and one fictitious drug (Semeron) which was introduced to identify over-claimers. For each drug they have to select one of the answers: never used the drug, used it over a decade ago, or in the last decade, year, month, week, or day.\\nDatabase contains 18 classification problems. Each of independent label variables contains seven classes: \"Never Used\", \"Used over a Decade Ago\", \"Used in Last Decade\", \"Used in Last Year\", \"Used in Last Month\", \"Used in Last Week\", and \"Used in Last Day\".\\n\\nProblem which can be solved:\\n* Seven class classifications for each drug separately.\\n* Problem can be transformed to binary classification by union of part of classes into one new class. For example, \"Never Used\", \"Used over a Decade Ago\" form class \"Non-user\" and all other classes form class \"User\".\\n* The best binarization of classes for each attribute.\\n* Evaluation of risk to be drug consumer for each drug.\\n\\nDetailed description of database and process of data quantification are presented in E. Fehrman, A. K. Muhammad, E. M. Mirkes, V. Egan and A. N. Gorban, \"The Five Factor Model of personality and evaluation of drug consumption risk.,\" arXiv https://arxiv.org/abs/1506.06297, 2015\\nPaper above solve binary classification problem for all drugs. For most of drugs sensitivity and specificity are greater than 75%.\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '1. ID is number of record in original database. Cannot be related to participant. It can be used for reference only.\\r\\n\\r\\n2. Age (Real) is age of participant and has one of the values:\\r\\n     Value    Meaning Cases Fraction\\r\\n     -0.95197 18-24   643   34.11%\\r\\n     -0.07854 25-34   481   25.52%\\r\\n      0.49788 35-44   356   18.89%\\r\\n      1.09449 45-54   294   15.60%\\r\\n      1.82213 55-64    93    4.93%\\r\\n      2.59171 65+      18    0.95%\\r\\n     Descriptive statistics\\r\\n     Min      Max     Mean    Std.dev.\\r\\n     -0.95197 2.59171 0.03461 0.87813\\r\\n\\r\\n3. Gender (Real) is gender of participant:\\r\\n     Value    Meaning Cases Fraction\\r\\n      0.48246 Female  942   49.97%\\r\\n     -0.48246 Male    943   50.03%\\r\\n     Descriptive statistics\\r\\n     Min      Max     Mean     Std.dev.\\r\\n     -0.48246 0.48246 -0.00026 0.48246\\r\\n\\r\\n4. Education (Real) is level of education of participant and has one of the values:\\r\\n     Value    Meaning                                              Cases Fraction\\r\\n     -2.43591 Left school before 16 years                           28    1.49%\\r\\n     -1.73790 Left school at 16 years                               99    5.25%\\r\\n     -1.43719 Left school at 17 years                               30    1.59%\\r\\n     -1.22751 Left school at 18 years                              100    5.31%\\r\\n     -0.61113 Some college or university, no certificate or degree 506   26.84%\\r\\n     -0.05921 Professional certificate/ diploma                    270   14.32%\\r\\n      0.45468 University degree                                    480   25.46%\\r\\n      1.16365 Masters degree                                       283   15.01%\\r\\n      1.98437 Doctorate degree                                      89    4.72%\\r\\n     Descriptive statistics\\r\\n     Min      Max     Mean     Std.dev.\\r\\n     -2.43591 1.98437 -0.00379 0.95004\\r\\n\\r\\n5. Country (Real) is country of current residence of participant and has one of the values:\\r\\n     Value    Meaning             Cases Fraction\\r\\n     -0.09765 Australia             54   2.86%\\r\\n      0.24923 Canada                87   4.62%\\r\\n     -0.46841 New Zealand            5   0.27%\\r\\n     -0.28519 Other                118   6.26%\\r\\n      0.21128 Republic of Ireland   20   1.06%\\r\\n      0.96082 UK                  1044  55.38%\\r\\n     -0.57009 USA                  557  29.55%\\r\\n     Descriptive statistics\\r\\n     Min      Max     Mean    Std.dev.\\r\\n     -0.57009 0.96082 0.35554 0.70015\\r\\n\\r\\n6. Ethnicity (Real) is ethnicity of participant and has one of the values:\\r\\n     Value    Meaning           Cases Fraction\\r\\n     -0.50212 Asian               26   1.38%\\r\\n     -1.10702 Black               33   1.75%\\r\\n      1.90725 Mixed-Black/Asian    3   0.16%\\r\\n      0.12600 Mixed-White/Asian   20   1.06%\\r\\n     -0.22166 Mixed-White/Black   20   1.06%\\r\\n      0.11440 Other               63   3.34%\\r\\n     -0.31685 White             1720  91.25%\\r\\n     Descriptive statistics\\r\\n     Min      Max     Mean     Std.dev.\\r\\n     -1.10702 1.90725 -0.30958 0.16618\\r\\n\\r\\n7. Nscore (Real) is NEO-FFI-R Neuroticism. Possible values are presented in table below:\\r\\n     Nscore Cases Value         Nscore Cases Value         Nscore Cases Value\\r\\n     12      1    -3.46436      29     60    -0.67825      46     67    1.02119\\r\\n     13      1    -3.15735      30     61    -0.58016      47     27    1.13281\\r\\n     14      7    -2.75696      31     87    -0.46725      48     49    1.23461\\r\\n     15      4    -2.52197      32     78    -0.34799      49     40    1.37297\\r\\n     16      3    -2.42317      33     68    -0.24649      50     24    1.49158\\r\\n     17      4    -2.34360      34     76    -0.14882      51     27    1.60383\\r\\n     18     10    -2.21844      35     69    -0.05188      52     17    1.72012\\r\\n     19     16    -2.05048      36     73     0.04257      53     20    1.83990\\r\\n     20     24    -1.86962      37     67     0.13606      54     15    1.98437\\r\\n     21     31    -1.69163      38     63     0.22393      55     11    2.12700\\r\\n     22     26    -1.55078      39     66     0.31287      56     10    2.28554\\r\\n     23     29    -1.43907      40     80     0.41667      57      6    2.46262\\r\\n     24     35    -1.32828      41     61     0.52135      58      3    2.61139\\r\\n     25     56    -1.19430      42     77     0.62967      59      5    2.82196\\r\\n     26     57    -1.05308      43     49     0.73545      60      2    3.27393\\r\\n     27     65    -0.92104      44     51     0.82562\\r\\n     28     70    -0.79151      45     37     0.91093\\r\\n     Descriptive statistics\\r\\n     Min      Max     Mean    Std.dev.\\r\\n     -3.46436 3.27393 0.00004 0.99808\\r\\n\\r\\n8. Escore (Real) is NEO-FFI-R Extraversion. Possible values are presented in table below:\\r\\n     Escore Cases Value         Escore Cases Value         Escore Cases Value\\r\\n     16      2    -3.27393      31      55   -1.23177      45     91    0.80523\\r\\n     18      1    -3.00537      32      52   -1.09207      46     69    0.96248\\r\\n     19      6    -2.72827      33      77   -0.94779      47     64    1.11406\\r\\n     20      3    -2.53830      34      68   -0.80615      48     62    1.28610\\r\\n     21      3    -2.44904      35      58   -0.69509      49     37    1.45421\\r\\n     22      8    -2.32338      36      89   -0.57545      50     25    1.58487\\r\\n     23      5    -2.21069      37      90   -0.43999      51     34    1.74091\\r\\n     24      9    -2.11437      38     106   -0.30033      52     21    1.93886\\r\\n     25      4    -2.03972      39     107   -0.15487      53     15    2.12700\\r\\n     26     21    -1.92173      40     130    0.00332      54     10    2.32338\\r\\n     27     23    -1.76250      41     116    0.16767      55      9    2.57309\\r\\n     28     23    -1.63340      42     109    0.32197      56      2    2.85950\\r\\n     29     32    -1.50796      43     105    0.47617      58      1    3.00537\\r\\n     30     38    -1.37639      44     103    0.63779      59      2    3.27393\\r\\n     Descriptive statistics\\r\\n     Min      Max     Mean     Std.dev.\\r\\n     -3.27393 3.27393 -0.00016 0.99745\\r\\n\\r\\n9. Oscore (Real) is NEO-FFI-R Openness to experience. Possible values are presented in table below:\\r\\n     Oscore Cases Value         Oscore Cases Value         Oscore Cases Value\\r\\n     24      2    -3.27393      38      64   -1.11902      50     83    0.58331\\r\\n     26      4    -2.85950      39      60   -0.97631      51     87    0.72330\\r\\n     28      4    -2.63199      40      68   -0.84732      52     87    0.88309\\r\\n     29     11    -2.39883      41      76   -0.71727      53     81    1.06238\\r\\n     30      9    -2.21069      42      87   -0.58331      54     57    1.24033\\r\\n     31      9    -2.09015      43      86   -0.45174      55     63    1.43533\\r\\n     32     13    -1.97495      44     101   -0.31776      56     38    1.65653\\r\\n     33     23    -1.82919      45     103   -0.17779      57     34    1.88511\\r\\n     34     25    -1.68062      46     134   -0.01928      58     19    2.15324\\r\\n     35     26    -1.55521      47     107    0.14143      59     13    2.44904\\r\\n     36     39    -1.42424      48     116    0.29338      60      7    2.90161\\r\\n     37     51    -1.27553      49      98    0.44585\\r\\n     Descriptive statistics\\r\\n     Min      Max     Mean     Std.dev.\\r\\n     -3.27393 2.90161 -0.00053 0.99623\\r\\n\\r\\n10. Ascore (Real) is NEO-FFI-R Agreeableness. Possible values are presented in table below:\\r\\n     Ascore Cases Value         Ascore Cases Value         Ascore Cases Value\\r\\n     12      1    -3.46436      34      42   -1.34289      48     104   0.76096\\r\\n     16      1    -3.15735      35      45   -1.21213      49      85   0.94156\\r\\n     18      1    -3.00537      36      62   -1.07533      50      68   1.11406\\r\\n     23      1    -2.90161      37      83   -0.91699      51      58   1.2861\\r\\n     24      2    -2.78793      38      82   -0.76096      52      39   1.45039\\r\\n     25      1    -2.70172      39     102   -0.60633      53      36   1.61108\\r\\n     26      7    -2.53830      40      98   -0.45321      54      36   1.81866\\r\\n     27      7    -2.35413      41     114   -0.30172      55      16   2.03972\\r\\n     28      8    -2.21844      42     101   -0.15487      56      14   2.23427\\r\\n     29     13    -2.07848      43     105   -0.01729      57       8   2.46262\\r\\n     30     18    -1.92595      44     118    0.13136      58       7   2.75696\\r\\n     31     24    -1.77200      45     112    0.28783      59       1   3.15735\\r\\n     32     30    -1.62090      46     100    0.43852      60       1   3.46436\\r\\n     33     34    -1.47955      47     100    0.59042                  \\r\\n     Descriptive statistics\\r\\n     Min      Max     Mean     Std.dev.\\r\\n     -3.46436 3.46436 -0.00024 0.99744\\r\\n\\r\\n11. Cscore (Real) is NEO-FFI-R Conscientiousness. Possible values are presented in table below:\\r\\n     Cscore Cases Value         Cscore Cases Value         Cscore Cases Value\\r\\n     17      1    -3.46436      32       39  -1.25773      46     113   0.58489\\r\\n     19      1    -3.15735      33       49  -1.13788      47      95   0.7583\\r\\n     20      3    -2.90161      34       55  -1.01450      48      95   0.93949\\r\\n     21      2    -2.72827      35       55  -0.89891      49      76   1.13407\\r\\n     22      5    -2.57309      36       69  -0.78155      50      47   1.30612\\r\\n     23      5    -2.42317      37       81  -0.65253      51      43   1.46191\\r\\n     24      6    -2.30408      38       77  -0.52745      52      34   1.63088\\r\\n     25      9    -2.18109      39       87  -0.40581      53      28   1.81175\\r\\n     26     13    -2.04506      40       97  -0.27607      54      27   2.04506\\r\\n     27     13    -1.92173      41       99  -0.14277      55      13   2.33337\\r\\n     28     25    -1.78169      42      105  -0.00665      56       8   2.63199\\r\\n     29     24    -1.64101      43       90   0.12331      57       3   3.00537\\r\\n     30     29    -1.51840      44      111   0.25953      59       1   3.46436\\r\\n     31     41    -1.38502      45      111   0.41594                  \\r\\n     Descriptive statistics\\r\\n     Min      Max     Mean     Std.dev.\\r\\n     -3.46436 3.46436 -0.00039 0.99752\\r\\n\\r\\n12. Impulsive (Real) is impulsiveness measured by BIS-11. Possible values are presented in table below:\\r\\n     Impulsiveness Cases Fraction\\r\\n     -2.55524       20    1.06%\\r\\n     -1.37983      276   14.64%\\r\\n     -0.71126      307   16.29%\\r\\n     -0.21712      355   18.83%\\r\\n      0.19268      257   13.63%\\r\\n      0.52975      216   11.46%\\r\\n      0.88113      195   10.34%\\r\\n      1.29221      148    7.85%\\r\\n      1.86203      104    5.52%\\r\\n      2.90161        7    0.37%\\r\\n     Descriptive statistics\\r\\n     Min      Max     Mean    Std.dev.\\r\\n     -2.55524 2.90161 0.00721 0.95446\\r\\n\\r\\n13. SS (Real) is sensation seeing measured by ImpSS. Possible values are presented in table below:\\r\\n     SS       Cases Fraction\\r\\n     -2.07848  71    3.77%\\r\\n     -1.54858  87    4.62%\\r\\n     -1.18084 132    7.00%\\r\\n     -0.84637 169    8.97%\\r\\n     -0.52593 211   11.19%\\r\\n     -0.21575 223   11.83%\\r\\n      0.07987 219   11.62%\\r\\n      0.40148 249   13.21%\\r\\n      0.76540 211   11.19%\\r\\n      1.22470 210   11.14%\\r\\n      1.92173 103    5.46%\\r\\n     Descriptive statistics\\r\\n     Min      Max     Mean     Std.dev.\\r\\n     -2.07848 1.92173 -0.00329 0.96370\\r\\n\\r\\n14. Alcohol is class of alcohol consumption. It is output attribute with following distribution of classes.\\r\\n\\r\\n15. Amphet is class of amphetamines consumption. It is output attribute with following distribution of classes.\\r\\n\\r\\n16. Amyl is class of amyl nitrite consumption. It is output attribute with following distribution of classes.\\r\\n\\r\\n17. Benzos is class of benzodiazepine consumption. It is output attribute with following distribution of classes:\\r\\n     Value Class                     Alcohol        Amphet          Amyl          Benzos\\r\\n                                  Cases Fraction Cases Fraction Cases Fraction Cases Fraction\\r\\n     CL0   Never Used              34    1.80%   976   51.78%   1305  69.23%   1000  53.05%\\r\\n     CL1   Used over a Decade Ago  34    1.80%   230   12.20%    210  11.14%    116   6.15%\\r\\n     CL2   Used in Last Decade     68    3.61%   243   12.89%    237  12.57%    234  12.41%\\r\\n     CL3   Used in Last Year      198   10.50%   198   10.50%     92   4.88%    236  12.52%\\r\\n     CL4   Used in Last Month     287   15.23%    75    3.98%     24   1.27%    120   6.37%\\r\\n     CL5   Used in Last Week      759   40.27%    61    3.24%     14   0.74%     84   4.46%\\r\\n     CL6   Used in Last Day       505   26.79%   102    5.41%      3   0.16%     95   5.04%\\r\\n\\r\\n18. Caff is class of caffeine consumption. It is output attribute with following distribution of classes.\\r\\n\\r\\n19. Cannabis is class of cannabis consumption. It is output attribute with following distribution of classes.\\r\\n\\r\\n20. Choc is class of chocolate consumption. It is output attribute with following distribution of classes.\\r\\n\\r\\n21. Coke is class of cocaine consumption. It is output attribute with following distribution of classes:\\r\\n     Value Class                      Caff         Cannabis         Choc           Coke\\r\\n                                  Cases Fraction Cases Fraction Cases Fraction Cases Fraction\\r\\n     CL0   Never Used               27   1.43%   413   21.91%    32    1.70%   1038  55.07%\\r\\n     CL1   Used over a Decade Ago   10   0.53%   207   10.98%     3    0.16%    160   8.49%\\r\\n     CL2   Used in Last Decade      24   1.27%   266   14.11%    10    0.53%    270  14.32%\\r\\n     CL3   Used in Last Year        60   3.18%   211   11.19%    54    2.86%    258  13.69%\\r\\n     CL4   Used in Last Month      106   5.62%   140    7.43%   296   15.70%     99   5.25%\\r\\n     CL5   Used in Last Week       273  14.48%   185    9.81%   683   36.23%     41   2.18%\\r\\n     CL6   Used in Last Day       1385  73.47%   463   24.56%   807   42.81%     19   1.01%\\r\\n\\r\\n22. Crack is class of crack consumption. It is output attribute with following distribution of classes.\\r\\n\\r\\n23. Ecstasy is class of ecstasy consumption. It is output attribute with following distribution of classes.\\r\\n\\r\\n24. Heroin is class of heroin consumption. It is output attribute with following distribution of classes.\\r\\n\\r\\n25. Ketamine is class of ketamine consumption. It is output attribute with following distribution of classes:\\r\\n     Value Class                     Crack         Ecstasy         Heroin        Ketamine\\r\\n                                  Cases Fraction Cases Fraction Cases Fraction Cases Fraction\\r\\n     CL0   Never Used             1627  86.31%   1021  54.16%   1605  85.15%   1490  79.05%\\r\\n     CL1   Used over a Decade Ago   67   3.55%    113   5.99%     68   3.61%     45   2.39%\\r\\n     CL2   Used in Last Decade     112   5.94%    234  12.41%     94   4.99%    142   7.53%\\r\\n     CL3   Used in Last Year        59   3.13%    277  14.69%     65   3.45%    129   6.84%\\r\\n     CL4   Used in Last Month        9   0.48%    156   8.28%     24   1.27%     42   2.23%\\r\\n     CL5   Used in Last Week         9   0.48%     63   3.34%     16   0.85%     33   1.75%\\r\\n     CL6   Used in Last Day          2   0.11%     21   1.11%     13   0.69%      4   0.21%\\r\\n\\r\\n26. Legalh is class of legal highs consumption. It is output attribute with following distribution of classes\\r\\n\\r\\n27. LSD is class of alcohol consumption. It is output attribute with following distribution of classes\\r\\n\\r\\n28. Meth is class of methadone consumption. It is output attribute with following distribution of classes.\\r\\n\\r\\n29. Mushrooms is class of magic mushrooms consumption. It is output attribute with following distribution of classes:\\r\\n     Value Class                     Legalh          LSD            Meth         Mushrooms\\r\\n                                  Cases Fraction Cases Fraction Cases Fraction Cases Fraction\\r\\n     CL0   Never Used             1094  58.04%   1069  56.71%   1429  75.81%   982   52.10%\\r\\n     CL1   Used over a Decade Ago   29   1.54%    259  13.74%     39   2.07%   209   11.09%\\r\\n     CL2   Used in Last Decade     198  10.50%    177   9.39%     97   5.15%   260   13.79%\\r\\n     CL3   Used in Last Year       323  17.14%    214  11.35%    149   7.90%   275   14.59%\\r\\n     CL4   Used in Last Month      110   5.84%     97   5.15%     50   2.65%   115    6.10%\\r\\n     CL5   Used in Last Week        64   3.40%     56   2.97%     48   2.55%    40    2.12%\\r\\n     CL6   Used in Last Day         67   3.55%     13   0.69%     73   3.87%     4    0.21%\\r\\n\\r\\n30. Nicotine is class of nicotine consumption. It is output attribute with following distribution of classes.\\r\\n\\r\\n31. Semer is class of fictitious drug Semeron consumption. It is output attribute with following distribution of classes.\\r\\n\\r\\n32. VSA is class of volatile substance abuse consumption. It is output attribute with following distribution of classes:\\r\\n     Value Class                    Nicotine        Semer           VSA\\r\\n                                  Cases Fraction Cases Fraction Cases Fraction\\r\\n     CL0   Never Used             428   22.71%   1877  99.58%   1455  77.19%\\r\\n     CL1   Used over a Decade Ago 193   10.24%      2   0.11%    200  10.61%\\r\\n     CL2   Used in Last Decade    204   10.82%      3   0.16%    135   7.16%\\r\\n     CL3   Used in Last Year      185    9.81%      2   0.11%     61   3.24%\\r\\n     CL4   Used in Last Month     108    5.73%      1   0.05%     13   0.69%\\r\\n     CL5   Used in Last Week      157    8.33%      0   0.00%     14   0.74%\\r\\n     CL6   Used in Last Day       610   32.36%      0   0.00%      7   0.37%\\r\\n', 'citation': None}}\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "drug_consumption_quantified = fetch_ucirepo(id=373) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_drug_original = drug_consumption_quantified.data.features \n",
    "y_drug_orginal = drug_consumption_quantified.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(drug_consumption_quantified.metadata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         name     role         type      demographic description units  \\\n",
      "0          id       ID      Integer             None        None  None   \n",
      "1         age  Feature   Continuous              Age        None  None   \n",
      "2      gender  Feature   Continuous           Gender        None  None   \n",
      "3   education  Feature   Continuous  Education Level        None  None   \n",
      "4     country  Feature   Continuous      Nationality        None  None   \n",
      "5   ethnicity  Feature   Continuous        Ethnicity        None  None   \n",
      "6      nscore  Feature   Continuous             None        None  None   \n",
      "7      escore  Feature   Continuous             None        None  None   \n",
      "8      oscore  Feature   Continuous             None        None  None   \n",
      "9      ascore  Feature   Continuous             None        None  None   \n",
      "10     cscore  Feature   Continuous             None        None  None   \n",
      "11  impuslive  Feature   Continuous             None        None  None   \n",
      "12         ss  Feature   Continuous             None        None  None   \n",
      "13    alcohol   Target  Categorical             None        None  None   \n",
      "14     amphet   Target  Categorical             None        None  None   \n",
      "15       amyl   Target  Categorical             None        None  None   \n",
      "16     benzos   Target  Categorical             None        None  None   \n",
      "17       caff   Target  Categorical             None        None  None   \n",
      "18   cannabis   Target  Categorical             None        None  None   \n",
      "19       choc   Target  Categorical             None        None  None   \n",
      "20       coke   Target  Categorical             None        None  None   \n",
      "21      crack   Target  Categorical             None        None  None   \n",
      "22    ecstasy   Target  Categorical             None        None  None   \n",
      "23     heroin   Target  Categorical             None        None  None   \n",
      "24   ketamine   Target  Categorical             None        None  None   \n",
      "25     legalh   Target  Categorical             None        None  None   \n",
      "26        lsd   Target  Categorical             None        None  None   \n",
      "27       meth   Target  Categorical             None        None  None   \n",
      "28  mushrooms   Target  Categorical             None        None  None   \n",
      "29   nicotine   Target  Categorical             None        None  None   \n",
      "30      semer   Target  Categorical             None        None  None   \n",
      "31        vsa   Target  Categorical             None        None  None   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n",
      "13             no  \n",
      "14             no  \n",
      "15             no  \n",
      "16             no  \n",
      "17             no  \n",
      "18             no  \n",
      "19             no  \n",
      "20             no  \n",
      "21             no  \n",
      "22             no  \n",
      "23             no  \n",
      "24             no  \n",
      "25             no  \n",
      "26             no  \n",
      "27             no  \n",
      "28             no  \n",
      "29             no  \n",
      "30             no  \n",
      "31             no  \n"
     ]
    }
   ],
   "source": [
    "# variable information \n",
    "print(drug_consumption_quantified.variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No need for handling missing value.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>country</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>nscore</th>\n",
       "      <th>escore</th>\n",
       "      <th>oscore</th>\n",
       "      <th>ascore</th>\n",
       "      <th>cscore</th>\n",
       "      <th>impuslive</th>\n",
       "      <th>ss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>0.12600</td>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>-0.21712</td>\n",
       "      <td>-1.18084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.07854</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>1.98437</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>-0.71126</td>\n",
       "      <td>-0.21575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>-1.37983</td>\n",
       "      <td>0.40148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.16365</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>-1.37983</td>\n",
       "      <td>-1.18084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.98437</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>-0.21712</td>\n",
       "      <td>-0.21575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age   gender  education  country  ethnicity   nscore   escore   oscore  \\\n",
       "0  0.49788  0.48246   -0.05921  0.96082    0.12600  0.31287 -0.57545 -0.58331   \n",
       "1 -0.07854 -0.48246    1.98437  0.96082   -0.31685 -0.67825  1.93886  1.43533   \n",
       "2  0.49788 -0.48246   -0.05921  0.96082   -0.31685 -0.46725  0.80523 -0.84732   \n",
       "3 -0.95197  0.48246    1.16365  0.96082   -0.31685 -0.14882 -0.80615 -0.01928   \n",
       "4  0.49788  0.48246    1.98437  0.96082   -0.31685  0.73545 -1.63340 -0.45174   \n",
       "\n",
       "    ascore   cscore  impuslive       ss  \n",
       "0 -0.91699 -0.00665   -0.21712 -1.18084  \n",
       "1  0.76096 -0.14277   -0.71126 -0.21575  \n",
       "2 -1.62090 -1.01450   -1.37983  0.40148  \n",
       "3  0.59042  0.58489   -1.37983 -1.18084  \n",
       "4 -0.30172  1.30612   -0.21712 -0.21575  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_drug_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have to map the numeric values of the demographic features to the right categorical variable to understand the dataset better**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>amphet</th>\n",
       "      <th>amyl</th>\n",
       "      <th>benzos</th>\n",
       "      <th>caff</th>\n",
       "      <th>cannabis</th>\n",
       "      <th>choc</th>\n",
       "      <th>coke</th>\n",
       "      <th>crack</th>\n",
       "      <th>ecstasy</th>\n",
       "      <th>heroin</th>\n",
       "      <th>ketamine</th>\n",
       "      <th>legalh</th>\n",
       "      <th>lsd</th>\n",
       "      <th>meth</th>\n",
       "      <th>mushrooms</th>\n",
       "      <th>nicotine</th>\n",
       "      <th>semer</th>\n",
       "      <th>vsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CL5</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CL5</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL5</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CL4</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  alcohol amphet amyl benzos caff cannabis choc coke crack ecstasy heroin  \\\n",
       "0     CL5    CL2  CL0    CL2  CL6      CL0  CL5  CL0   CL0     CL0    CL0   \n",
       "1     CL5    CL2  CL2    CL0  CL6      CL4  CL6  CL3   CL0     CL4    CL0   \n",
       "2     CL6    CL0  CL0    CL0  CL6      CL3  CL4  CL0   CL0     CL0    CL0   \n",
       "3     CL4    CL0  CL0    CL3  CL5      CL2  CL4  CL2   CL0     CL0    CL0   \n",
       "4     CL4    CL1  CL1    CL0  CL6      CL3  CL6  CL0   CL0     CL1    CL0   \n",
       "\n",
       "  ketamine legalh  lsd meth mushrooms nicotine semer  vsa  \n",
       "0      CL0    CL0  CL0  CL0       CL0      CL2   CL0  CL0  \n",
       "1      CL2    CL0  CL2  CL3       CL0      CL4   CL0  CL0  \n",
       "2      CL0    CL0  CL0  CL0       CL1      CL0   CL0  CL0  \n",
       "3      CL2    CL0  CL0  CL0       CL0      CL2   CL0  CL0  \n",
       "4      CL0    CL1  CL0  CL0       CL2      CL2   CL0  CL0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_drug_orginal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need to map these identifiers to the right meaning too, according to the dataset description.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>country</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>nscore</th>\n",
       "      <th>escore</th>\n",
       "      <th>oscore</th>\n",
       "      <th>ascore</th>\n",
       "      <th>cscore</th>\n",
       "      <th>...</th>\n",
       "      <th>ecstasy</th>\n",
       "      <th>heroin</th>\n",
       "      <th>ketamine</th>\n",
       "      <th>legalh</th>\n",
       "      <th>lsd</th>\n",
       "      <th>meth</th>\n",
       "      <th>mushrooms</th>\n",
       "      <th>nicotine</th>\n",
       "      <th>semer</th>\n",
       "      <th>vsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>0.12600</td>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.07854</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>1.98437</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>...</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.16365</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.98437</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>...</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age   gender  education  country  ethnicity   nscore   escore   oscore  \\\n",
       "0  0.49788  0.48246   -0.05921  0.96082    0.12600  0.31287 -0.57545 -0.58331   \n",
       "1 -0.07854 -0.48246    1.98437  0.96082   -0.31685 -0.67825  1.93886  1.43533   \n",
       "2  0.49788 -0.48246   -0.05921  0.96082   -0.31685 -0.46725  0.80523 -0.84732   \n",
       "3 -0.95197  0.48246    1.16365  0.96082   -0.31685 -0.14882 -0.80615 -0.01928   \n",
       "4  0.49788  0.48246    1.98437  0.96082   -0.31685  0.73545 -1.63340 -0.45174   \n",
       "\n",
       "    ascore   cscore  ...  ecstasy  heroin ketamine legalh  lsd meth mushrooms  \\\n",
       "0 -0.91699 -0.00665  ...      CL0     CL0      CL0    CL0  CL0  CL0       CL0   \n",
       "1  0.76096 -0.14277  ...      CL4     CL0      CL2    CL0  CL2  CL3       CL0   \n",
       "2 -1.62090 -1.01450  ...      CL0     CL0      CL0    CL0  CL0  CL0       CL1   \n",
       "3  0.59042  0.58489  ...      CL0     CL0      CL2    CL0  CL0  CL0       CL0   \n",
       "4 -0.30172  1.30612  ...      CL1     CL0      CL0    CL1  CL0  CL0       CL2   \n",
       "\n",
       "  nicotine semer  vsa  \n",
       "0      CL2   CL0  CL0  \n",
       "1      CL4   CL0  CL0  \n",
       "2      CL0   CL0  CL0  \n",
       "3      CL2   CL0  CL0  \n",
       "4      CL2   CL0  CL0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate features and target into a single DataFrame\n",
    "drug_complete = pd.concat([X_drug_original, y_drug_orginal], axis=1)\n",
    "\n",
    "# Display the first few rows of the combined dataset\n",
    "drug_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    1885\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate values\n",
    "print(drug_complete.duplicated().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No need to handle duplicate values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Mapping the features to categorical meanings for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionaries based on the unique values\n",
    "age_columns = {\n",
    "    -0.95: '18 - 24',\n",
    "    -0.08: '25 - 34',\n",
    "    0.50: '35 - 44',\n",
    "    1.09: '45 - 54',\n",
    "    1.82: '55 - 64',\n",
    "    2.59: '65+'\n",
    "}\n",
    "\n",
    "gender_columns = {\n",
    "    0.48: 'Female',\n",
    "    -0.48: 'Male'\n",
    "            }\n",
    "\n",
    "education_columns = {\n",
    "    -2.44: 'Left school before 16 years',\n",
    "    -1.74: 'Left school at 16 years',\n",
    "    -1.44: 'Left school at 17 years',\n",
    "    -1.23: 'Left school at 18 years',\n",
    "    -0.61: 'Some college or university, no certificate or degree',\n",
    "    -0.06: 'Professional certificate/ diploma',\n",
    "    0.45: 'University degree',\n",
    "    1.16: 'Masters degree',\n",
    "    1.98: 'Doctorate degree'\n",
    "}\n",
    "\n",
    "country_columns = {\n",
    "    -0.10: 'UK',\n",
    "    0.25: 'USA',\n",
    "    0.21: 'New Zealand',\n",
    "    0.96: 'Other',\n",
    "    -0.47: 'Australia',\n",
    "    -0.29: 'Ireland',\n",
    "    0.21: 'Canada'\n",
    "}\n",
    "\n",
    "ethnicity_columns = {\n",
    "    -0.50: 'Asian',\n",
    "    -1.11: 'Black',\n",
    "    1.91: 'Mixed-Black/Asian',\n",
    "    0.13: 'Mixed-White/Asian',\n",
    "    -0.22: 'Mixed-White/Black',\n",
    "    0.11: 'Other',\n",
    "    -0.32: 'White'\n",
    "}\n",
    "\n",
    "# Round the values and replace them using the mapping dictionaries\n",
    "drug_complete['age'] = drug_complete['age'].round(2).replace(age_columns)\n",
    "drug_complete['gender'] = drug_complete['gender'].round(2).replace(gender_columns)\n",
    "drug_complete['education'] = drug_complete['education'].round(2).replace(education_columns)\n",
    "drug_complete['country'] = drug_complete['country'].round(2).replace(country_columns)\n",
    "drug_complete['ethnicity'] = drug_complete['ethnicity'].round(2).replace(ethnicity_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption_columns = {\n",
    "    'CL0': 'Never Used',\n",
    "    'CL1': 'Used over a Decade Ago',\n",
    "    'CL2': 'Used in Last Decade',\n",
    "    'CL3': 'Used in Last Year',\n",
    "    'CL4': 'Used in Last Month',\n",
    "    'CL5': 'Used in Last Week',\n",
    "    'CL6': 'Used in Last Day',\n",
    "    }\n",
    "drug_complete['alcohol'] = drug_complete['alcohol'].replace(consumption_columns)\n",
    "drug_complete['amphet'] = drug_complete['amphet'].replace(consumption_columns)\n",
    "drug_complete['amyl'] = drug_complete['amyl'].replace(consumption_columns)\n",
    "drug_complete['benzos'] = drug_complete['benzos'].replace(consumption_columns)\n",
    "drug_complete['caff'] = drug_complete['caff'].replace(consumption_columns)\n",
    "drug_complete['cannabis'] = drug_complete['cannabis'].replace(consumption_columns)\n",
    "drug_complete['choc'] = drug_complete['choc'].replace(consumption_columns)\n",
    "drug_complete['coke'] = drug_complete['coke'].replace(consumption_columns)\n",
    "drug_complete['crack'] = drug_complete['crack'].replace(consumption_columns)\n",
    "drug_complete['ecstasy'] = drug_complete['ecstasy'].replace(consumption_columns)\n",
    "drug_complete['heroin'] = drug_complete['heroin'].replace(consumption_columns)\n",
    "drug_complete['ketamine'] = drug_complete['ketamine'].replace(consumption_columns)\n",
    "drug_complete['legalh'] = drug_complete['legalh'].replace(consumption_columns)\n",
    "drug_complete['lsd'] = drug_complete['lsd'].replace(consumption_columns)\n",
    "drug_complete['meth'] = drug_complete['meth'].replace(consumption_columns)\n",
    "drug_complete['mushrooms'] = drug_complete['mushrooms'].replace(consumption_columns)\n",
    "drug_complete['nicotine'] = drug_complete['nicotine'].replace(consumption_columns)\n",
    "drug_complete['semer'] = drug_complete['semer'].replace(consumption_columns)\n",
    "drug_complete['vsa'] = drug_complete['vsa'].replace(consumption_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>country</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>nscore</th>\n",
       "      <th>escore</th>\n",
       "      <th>oscore</th>\n",
       "      <th>ascore</th>\n",
       "      <th>cscore</th>\n",
       "      <th>...</th>\n",
       "      <th>ecstasy</th>\n",
       "      <th>heroin</th>\n",
       "      <th>ketamine</th>\n",
       "      <th>legalh</th>\n",
       "      <th>lsd</th>\n",
       "      <th>meth</th>\n",
       "      <th>mushrooms</th>\n",
       "      <th>nicotine</th>\n",
       "      <th>semer</th>\n",
       "      <th>vsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35 - 44</td>\n",
       "      <td>Female</td>\n",
       "      <td>Professional certificate/ diploma</td>\n",
       "      <td>Other</td>\n",
       "      <td>Mixed-White/Asian</td>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>...</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25 - 34</td>\n",
       "      <td>Male</td>\n",
       "      <td>Doctorate degree</td>\n",
       "      <td>Other</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>...</td>\n",
       "      <td>Used in Last Month</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Used in Last Year</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Month</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35 - 44</td>\n",
       "      <td>Male</td>\n",
       "      <td>Professional certificate/ diploma</td>\n",
       "      <td>Other</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>...</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used over a Decade Ago</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18 - 24</td>\n",
       "      <td>Female</td>\n",
       "      <td>Masters degree</td>\n",
       "      <td>Other</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>...</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35 - 44</td>\n",
       "      <td>Female</td>\n",
       "      <td>Doctorate degree</td>\n",
       "      <td>Other</td>\n",
       "      <td>White</td>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>...</td>\n",
       "      <td>Used over a Decade Ago</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used over a Decade Ago</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  gender                          education country  \\\n",
       "0  35 - 44  Female  Professional certificate/ diploma   Other   \n",
       "1  25 - 34    Male                   Doctorate degree   Other   \n",
       "2  35 - 44    Male  Professional certificate/ diploma   Other   \n",
       "3  18 - 24  Female                     Masters degree   Other   \n",
       "4  35 - 44  Female                   Doctorate degree   Other   \n",
       "\n",
       "           ethnicity   nscore   escore   oscore   ascore   cscore  ...  \\\n",
       "0  Mixed-White/Asian  0.31287 -0.57545 -0.58331 -0.91699 -0.00665  ...   \n",
       "1              White -0.67825  1.93886  1.43533  0.76096 -0.14277  ...   \n",
       "2              White -0.46725  0.80523 -0.84732 -1.62090 -1.01450  ...   \n",
       "3              White -0.14882 -0.80615 -0.01928  0.59042  0.58489  ...   \n",
       "4              White  0.73545 -1.63340 -0.45174 -0.30172  1.30612  ...   \n",
       "\n",
       "                  ecstasy      heroin             ketamine  \\\n",
       "0              Never Used  Never Used           Never Used   \n",
       "1      Used in Last Month  Never Used  Used in Last Decade   \n",
       "2              Never Used  Never Used           Never Used   \n",
       "3              Never Used  Never Used  Used in Last Decade   \n",
       "4  Used over a Decade Ago  Never Used           Never Used   \n",
       "\n",
       "                   legalh                  lsd               meth  \\\n",
       "0              Never Used           Never Used         Never Used   \n",
       "1              Never Used  Used in Last Decade  Used in Last Year   \n",
       "2              Never Used           Never Used         Never Used   \n",
       "3              Never Used           Never Used         Never Used   \n",
       "4  Used over a Decade Ago           Never Used         Never Used   \n",
       "\n",
       "                mushrooms             nicotine       semer         vsa  \n",
       "0              Never Used  Used in Last Decade  Never Used  Never Used  \n",
       "1              Never Used   Used in Last Month  Never Used  Never Used  \n",
       "2  Used over a Decade Ago           Never Used  Never Used  Never Used  \n",
       "3              Never Used  Used in Last Decade  Never Used  Never Used  \n",
       "4     Used in Last Decade  Used in Last Decade  Never Used  Never Used  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Differentiate between active a drug users and non-drug users\n",
    "\n",
    "We do this according to the original study, referring to \"Never used\", \"Used over a decade ago\" and \"Used in last decade\" as *non-users* and everyone that used the drug in the last year up to the last day as *active drug users*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>country</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>nscore</th>\n",
       "      <th>escore</th>\n",
       "      <th>oscore</th>\n",
       "      <th>ascore</th>\n",
       "      <th>cscore</th>\n",
       "      <th>...</th>\n",
       "      <th>ecstasy</th>\n",
       "      <th>heroin</th>\n",
       "      <th>ketamine</th>\n",
       "      <th>legalh</th>\n",
       "      <th>lsd</th>\n",
       "      <th>meth</th>\n",
       "      <th>mushrooms</th>\n",
       "      <th>nicotine</th>\n",
       "      <th>semer</th>\n",
       "      <th>vsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35 - 44</td>\n",
       "      <td>Female</td>\n",
       "      <td>Professional certificate/ diploma</td>\n",
       "      <td>Other</td>\n",
       "      <td>Mixed-White/Asian</td>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25 - 34</td>\n",
       "      <td>Male</td>\n",
       "      <td>Doctorate degree</td>\n",
       "      <td>Other</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35 - 44</td>\n",
       "      <td>Male</td>\n",
       "      <td>Professional certificate/ diploma</td>\n",
       "      <td>Other</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18 - 24</td>\n",
       "      <td>Female</td>\n",
       "      <td>Masters degree</td>\n",
       "      <td>Other</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35 - 44</td>\n",
       "      <td>Female</td>\n",
       "      <td>Doctorate degree</td>\n",
       "      <td>Other</td>\n",
       "      <td>White</td>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  gender                          education country  \\\n",
       "0  35 - 44  Female  Professional certificate/ diploma   Other   \n",
       "1  25 - 34    Male                   Doctorate degree   Other   \n",
       "2  35 - 44    Male  Professional certificate/ diploma   Other   \n",
       "3  18 - 24  Female                     Masters degree   Other   \n",
       "4  35 - 44  Female                   Doctorate degree   Other   \n",
       "\n",
       "           ethnicity   nscore   escore   oscore   ascore   cscore  ...  \\\n",
       "0  Mixed-White/Asian  0.31287 -0.57545 -0.58331 -0.91699 -0.00665  ...   \n",
       "1              White -0.67825  1.93886  1.43533  0.76096 -0.14277  ...   \n",
       "2              White -0.46725  0.80523 -0.84732 -1.62090 -1.01450  ...   \n",
       "3              White -0.14882 -0.80615 -0.01928  0.59042  0.58489  ...   \n",
       "4              White  0.73545 -1.63340 -0.45174 -0.30172  1.30612  ...   \n",
       "\n",
       "   ecstasy  heroin  ketamine  legalh  lsd  meth  mushrooms  nicotine  semer  \\\n",
       "0        0       0         0       0    0     0          0         0      0   \n",
       "1        1       0         0       0    0     1          0         1      0   \n",
       "2        0       0         0       0    0     0          0         0      0   \n",
       "3        0       0         0       0    0     0          0         0      0   \n",
       "4        0       0         0       0    0     0          0         0      0   \n",
       "\n",
       "   vsa  \n",
       "0    0  \n",
       "1    0  \n",
       "2    0  \n",
       "3    0  \n",
       "4    0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to map the drug use categories to 0 and 1\n",
    "def map_drug_usage(usage):\n",
    "    if usage in [\"Never Used\", \"Used over a Decade Ago\", \"Used in Last Decade\"]:\n",
    "        return 0\n",
    "    elif usage in [\"Used in Last Year\", \"Used in Last Month\", \"Used in Last Week\", \"Used in Last Day\"]:\n",
    "        return 1\n",
    "    else:\n",
    "        return usage  # Return the value unchanged if it doesn't match any category\n",
    "\n",
    "# List of drug-related columns\n",
    "drug_columns = [\"alcohol\", \"amphet\", \"amyl\", \"benzos\", \"caff\", \"cannabis\", \"choc\", \"coke\", \"crack\", \"ecstasy\",\n",
    "                \"heroin\", \"ketamine\", \"legalh\", \"lsd\", \"meth\", \"mushrooms\", \"nicotine\", \"semer\", \"vsa\"]\n",
    "\n",
    "# Apply the mapping function to each drug-related column\n",
    "for column in drug_columns:\n",
    "    drug_complete[column] = drug_complete[column].apply(map_drug_usage)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "drug_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'gender', 'education', 'country', 'ethnicity', 'nscore',\n",
      "       'escore', 'oscore', 'ascore', 'cscore', 'impuslive', 'ss', 'alcohol',\n",
      "       'amphet', 'amyl', 'benzos', 'caff', 'cannabis', 'choc', 'coke', 'crack',\n",
      "       'ecstasy', 'heroin', 'ketamine', 'legalh', 'lsd', 'meth', 'mushrooms',\n",
      "       'nicotine', 'semer', 'vsa'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display the column names to check if we handled all features successfully\n",
    "print(drug_complete.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Create drug categories to use as target variables for prediction\n",
    "\n",
    "For this proof of concept, we want to have some drug categories. To be precise \"legal light drugs\" will be alcohol and nicotine, \"party drugs\" will be coke, ecstasy and amphet, \"psychedelic drugs\" will be lsd and mushrooms and \"heavy drugs\" will be meth, heroin and crack. As long as just one of the substances has been used, the whole category will be marked as \"1\" / active user. If not a single substance of the respective category has been used, then the category will be of course \"0\".\n",
    "\n",
    "We do this because we assume that training models for prediction of a single drug will be harder, and the main goal remains bias identification and mitigation in this project. Creating groups however is in line with the original study; however they created different groups with different substances for their data exploration and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>country</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>nscore</th>\n",
       "      <th>escore</th>\n",
       "      <th>oscore</th>\n",
       "      <th>ascore</th>\n",
       "      <th>cscore</th>\n",
       "      <th>impuslive</th>\n",
       "      <th>ss</th>\n",
       "      <th>legal_light_drugs</th>\n",
       "      <th>party_drugs</th>\n",
       "      <th>psychedelic_drugs</th>\n",
       "      <th>heavy_drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35 - 44</td>\n",
       "      <td>Female</td>\n",
       "      <td>Professional certificate/ diploma</td>\n",
       "      <td>Other</td>\n",
       "      <td>Mixed-White/Asian</td>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>-0.21712</td>\n",
       "      <td>-1.18084</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25 - 34</td>\n",
       "      <td>Male</td>\n",
       "      <td>Doctorate degree</td>\n",
       "      <td>Other</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>-0.71126</td>\n",
       "      <td>-0.21575</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35 - 44</td>\n",
       "      <td>Male</td>\n",
       "      <td>Professional certificate/ diploma</td>\n",
       "      <td>Other</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>-1.37983</td>\n",
       "      <td>0.40148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18 - 24</td>\n",
       "      <td>Female</td>\n",
       "      <td>Masters degree</td>\n",
       "      <td>Other</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>-1.37983</td>\n",
       "      <td>-1.18084</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35 - 44</td>\n",
       "      <td>Female</td>\n",
       "      <td>Doctorate degree</td>\n",
       "      <td>Other</td>\n",
       "      <td>White</td>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>-0.21712</td>\n",
       "      <td>-0.21575</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  gender                          education country  \\\n",
       "0  35 - 44  Female  Professional certificate/ diploma   Other   \n",
       "1  25 - 34    Male                   Doctorate degree   Other   \n",
       "2  35 - 44    Male  Professional certificate/ diploma   Other   \n",
       "3  18 - 24  Female                     Masters degree   Other   \n",
       "4  35 - 44  Female                   Doctorate degree   Other   \n",
       "\n",
       "           ethnicity   nscore   escore   oscore   ascore   cscore  impuslive  \\\n",
       "0  Mixed-White/Asian  0.31287 -0.57545 -0.58331 -0.91699 -0.00665   -0.21712   \n",
       "1              White -0.67825  1.93886  1.43533  0.76096 -0.14277   -0.71126   \n",
       "2              White -0.46725  0.80523 -0.84732 -1.62090 -1.01450   -1.37983   \n",
       "3              White -0.14882 -0.80615 -0.01928  0.59042  0.58489   -1.37983   \n",
       "4              White  0.73545 -1.63340 -0.45174 -0.30172  1.30612   -0.21712   \n",
       "\n",
       "        ss  legal_light_drugs  party_drugs  psychedelic_drugs  heavy_drugs  \n",
       "0 -1.18084                  1            0                  0            0  \n",
       "1 -0.21575                  1            1                  0            1  \n",
       "2  0.40148                  1            0                  0            0  \n",
       "3 -1.18084                  1            0                  0            0  \n",
       "4 -0.21575                  1            0                  0            0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the categories and their corresponding columns\n",
    "new_drug_categories = {\n",
    "    \"legal_light_drugs\": [\"alcohol\", \"nicotine\"],\n",
    "    \"party_drugs\": [\"coke\", \"ecstasy\", \"amphet\"],\n",
    "    \"psychedelic_drugs\": [\"lsd\", \"mushrooms\"],\n",
    "    \"heavy_drugs\": [\"meth\", \"heroin\", \"crack\"]\n",
    "}\n",
    "\n",
    "# Create new columns for each category\n",
    "for category, columns in new_drug_categories.items():\n",
    "    drug_complete[category] = drug_complete[columns].max(axis=1)\n",
    "\n",
    "# Drop the original drug use information columns\n",
    "drug_complete.drop(columns=drug_columns, inplace=True)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "drug_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 One-Hot Encoding categorical features\n",
    "\n",
    "... for our machine learning models to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to numeric using one-hot encoding\n",
    "categorical_columns = ['age', 'gender', 'education', 'country', 'ethnicity']\n",
    "drug_complete = pd.get_dummies(drug_complete, columns=categorical_columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nscore</th>\n",
       "      <th>escore</th>\n",
       "      <th>oscore</th>\n",
       "      <th>ascore</th>\n",
       "      <th>cscore</th>\n",
       "      <th>impuslive</th>\n",
       "      <th>ss</th>\n",
       "      <th>legal_light_drugs</th>\n",
       "      <th>party_drugs</th>\n",
       "      <th>psychedelic_drugs</th>\n",
       "      <th>...</th>\n",
       "      <th>country_Ireland</th>\n",
       "      <th>country_Other</th>\n",
       "      <th>country_UK</th>\n",
       "      <th>country_USA</th>\n",
       "      <th>ethnicity_Black</th>\n",
       "      <th>ethnicity_Mixed-Black/Asian</th>\n",
       "      <th>ethnicity_Mixed-White/Asian</th>\n",
       "      <th>ethnicity_Mixed-White/Black</th>\n",
       "      <th>ethnicity_Other</th>\n",
       "      <th>ethnicity_White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>-0.21712</td>\n",
       "      <td>-1.18084</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>-0.71126</td>\n",
       "      <td>-0.21575</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>-1.37983</td>\n",
       "      <td>0.40148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>-1.37983</td>\n",
       "      <td>-1.18084</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>-0.21712</td>\n",
       "      <td>-0.21575</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    nscore   escore   oscore   ascore   cscore  impuslive       ss  \\\n",
       "0  0.31287 -0.57545 -0.58331 -0.91699 -0.00665   -0.21712 -1.18084   \n",
       "1 -0.67825  1.93886  1.43533  0.76096 -0.14277   -0.71126 -0.21575   \n",
       "2 -0.46725  0.80523 -0.84732 -1.62090 -1.01450   -1.37983  0.40148   \n",
       "3 -0.14882 -0.80615 -0.01928  0.59042  0.58489   -1.37983 -1.18084   \n",
       "4  0.73545 -1.63340 -0.45174 -0.30172  1.30612   -0.21712 -0.21575   \n",
       "\n",
       "   legal_light_drugs  party_drugs  psychedelic_drugs  ...  country_Ireland  \\\n",
       "0                  1            0                  0  ...                0   \n",
       "1                  1            1                  0  ...                0   \n",
       "2                  1            0                  0  ...                0   \n",
       "3                  1            0                  0  ...                0   \n",
       "4                  1            0                  0  ...                0   \n",
       "\n",
       "   country_Other  country_UK  country_USA  ethnicity_Black  \\\n",
       "0              1           0            0                0   \n",
       "1              1           0            0                0   \n",
       "2              1           0            0                0   \n",
       "3              1           0            0                0   \n",
       "4              1           0            0                0   \n",
       "\n",
       "   ethnicity_Mixed-Black/Asian  ethnicity_Mixed-White/Asian  \\\n",
       "0                            0                            1   \n",
       "1                            0                            0   \n",
       "2                            0                            0   \n",
       "3                            0                            0   \n",
       "4                            0                            0   \n",
       "\n",
       "   ethnicity_Mixed-White/Black  ethnicity_Other  ethnicity_White  \n",
       "0                            0                0                0  \n",
       "1                            0                0                1  \n",
       "2                            0                0                1  \n",
       "3                            0                0                1  \n",
       "4                            0                0                1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to drug_consumption_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Optional: save prepared dataset as csv\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'drug_consumption_processed.csv'\n",
    "drug_complete.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fairness Evaluation\n",
    "\n",
    "We will first explore the metrics for 3 different protected attributes on the original data with both AIF360 and DALEX toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Protected Attribute: Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label column\n",
    "label_column = 'heavy_drugs'  # Change this if you choose a different label column\n",
    "# Define the new protected attribute column after one-hot encoding\n",
    "protected_attribute = 'ethnicity_Black'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 AIF360 metrics for original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.explainers import MetricTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference: 0.1391779566725571\n",
      "Disparate Impact: 3.2964362850971924\n",
      "Consistency [0.8133687]\n",
      "Mean Difference 0.1391779566725571\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import StandardDataset\n",
    "\n",
    "# Create the StandardDataset for AIF360\n",
    "drug_complete_for_metrics = StandardDataset(drug_complete, label_name=label_column, favorable_classes=[1],\n",
    "                                protected_attribute_names=[protected_attribute], privileged_classes=[[1]])\n",
    "\n",
    "# Compute metrics for the original dataset\n",
    "metrics_for_original_data = BinaryLabelDatasetMetric(drug_complete_for_metrics,\n",
    "                                                     unprivileged_groups=[{protected_attribute: 0}],\n",
    "                                                     privileged_groups=[{protected_attribute: 1}])\n",
    "\n",
    "metric_explainer_original_data = MetricTextExplainer(metrics_for_original_data)\n",
    "\n",
    "# Print out the bias metrics for the original data\n",
    "print(\"Statistical Parity Difference:\", metrics_for_original_data.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metrics_for_original_data.disparate_impact())\n",
    "print(\"Consistency\", metrics_for_original_data.consistency())\n",
    "print(\"Mean Difference\", metrics_for_original_data.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 DALEX metrics for original data\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target variable\n",
    "X = drug_complete.drop(columns=[label_column])\n",
    "y = drug_complete[label_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Learn 2 different classifiers (Logistic Regression and Random Forest) on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "{'0': {'precision': 0.8793650793650793, 'recall': 0.9081967213114754, 'f1-score': 0.8935483870967742, 'support': 305.0}, '1': {'precision': 0.5483870967741935, 'recall': 0.4722222222222222, 'f1-score': 0.5074626865671641, 'support': 72.0}, 'accuracy': 0.8249336870026526, 'macro avg': {'precision': 0.7138760880696364, 'recall': 0.6902094717668488, 'f1-score': 0.7005055368319691, 'support': 377.0}, 'weighted avg': {'precision': 0.8161544301700031, 'recall': 0.8249336870026526, 'f1-score': 0.8198131869956286, 'support': 377.0}}\n",
      "Random Forest Classification Report:\n",
      "{'0': {'precision': 0.8699690402476781, 'recall': 0.921311475409836, 'f1-score': 0.8949044585987262, 'support': 305.0}, '1': {'precision': 0.5555555555555556, 'recall': 0.4166666666666667, 'f1-score': 0.4761904761904762, 'support': 72.0}, 'accuracy': 0.8249336870026526, 'macro avg': {'precision': 0.7127622979016168, 'recall': 0.6689890710382513, 'f1-score': 0.6855474673946013, 'support': 377.0}, 'weighted avg': {'precision': 0.8099219025876441, 'recall': 0.8249336870026526, 'f1-score': 0.8149378624889277, 'support': 377.0}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Train the classifiers\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# Train the classifiers\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_lr = lr_classifier.predict(X_test)\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "\n",
    "# Compute confusion matrices and classification reports\n",
    "conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "class_report_lr = classification_report(y_test, y_pred_lr, output_dict=True)\n",
    "\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "class_report_rf = classification_report(y_test, y_pred_rf, output_dict=True)\n",
    "\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(class_report_lr)\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(class_report_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Create Dataset for AIF360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Combine features and labels for AIF360\n",
    "train_data = X_train.copy()\n",
    "train_data[label_column] = y_train\n",
    "test_data = X_test.copy()\n",
    "test_data[label_column] = y_test\n",
    "\n",
    "# Convert the dataset to AIF360's BinaryLabelDataset format\n",
    "train_dataset = BinaryLabelDataset(df=train_data, label_names=[label_column], favorable_label=1, unfavorable_label=0, protected_attribute_names=[protected_attribute])\n",
    "test_dataset = BinaryLabelDataset(df=test_data, label_names=[label_column], favorable_label=1, unfavorable_label=0, protected_attribute_names=[protected_attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 AIF360 metrics for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Classification accuracy =  0.8249336870026526\n",
      "Equal Opportunity Difference (Test): 0.4788732394366197\n",
      "Average Odds Difference (Test): 0.2859482476252866\n",
      "Theil Index (Test): 0.1326496666723422\n",
      "Statistical Parity Difference/Mean Difference (Test): 0.16666666666666666\n",
      "Disparate Impact: inf\n",
      "Consistency: [0.79098143]\n",
      "Error Rate Difference: -0.025268817204301075\n",
      "False Negative Rate Difference: -0.47887323943661975\n",
      "False Positive Rate Difference: 0.09302325581395349\n",
      "Mean Difference 0.16666666666666666\n",
      "Binary Confusion Matrix: {'TP': 34.0, 'FP': 28.0, 'TN': 277.0, 'FN': 38.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "test_dataset_pred_lr = test_dataset.copy(deepcopy=True)\n",
    "test_dataset_pred_lr.labels = y_pred_lr.reshape(-1, 1)\n",
    "\n",
    "# Compute logistic regression fairness metrics\n",
    "metric_test_lr = ClassificationMetric(test_dataset, test_dataset_pred_lr, unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "metric_explainer_lr = MetricTextExplainer(metric_test_lr)\n",
    "\n",
    "print(\"LR Classification accuracy = \", metric_test_lr.accuracy())\n",
    "print(\"Equal Opportunity Difference (Test):\", metric_test_lr.equal_opportunity_difference())\n",
    "print(\"Average Odds Difference (Test):\", metric_test_lr.average_odds_difference())\n",
    "print(\"Theil Index (Test):\", metric_test_lr.theil_index())\n",
    "print(\"Statistical Parity Difference/Mean Difference (Test):\", metric_test_lr.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_test_lr.disparate_impact())\n",
    "print(\"Consistency:\", metric_test_lr.consistency())\n",
    "print(\"Error Rate Difference:\", metric_test_lr.error_rate_difference())\n",
    "print(\"False Negative Rate Difference:\", metric_test_lr.false_negative_rate_difference())\n",
    "print(\"False Positive Rate Difference:\", metric_test_lr.false_positive_rate_difference())\n",
    "print(\"Mean Difference\", metric_test_lr.mean_difference())\n",
    "print(\"Binary Confusion Matrix:\", metric_test_lr.binary_confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 AIF360 metrics for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Classification accuracy =  0.8249336870026526\n",
      "Equal Opportunity Difference (Test): 0.4225352112676056\n",
      "Average Odds Difference (Test): 0.2511347152683543\n",
      "Theil Index (Test): 0.14159985931315067\n",
      "Statistical Parity Difference/Mean Difference (Test): 0.14516129032258066\n",
      "Disparate Impact: inf\n",
      "Consistency: [0.79098143]\n",
      "Error Rate Difference: -0.025268817204301075\n",
      "False Negative Rate Difference: -0.4225352112676056\n",
      "False Positive Rate Difference: 0.07973421926910298\n",
      "Mean Difference 0.14516129032258066\n",
      "Binary Confusion Matrix: {'TP': 30.0, 'FP': 24.0, 'TN': 281.0, 'FN': 42.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "test_dataset_pred_rf = test_dataset.copy(deepcopy=True)\n",
    "test_dataset_pred_rf.labels = y_pred_rf.reshape(-1, 1)\n",
    "\n",
    "# Compute random forest fairness metrics\n",
    "metric_test_rf = ClassificationMetric(test_dataset, test_dataset_pred_rf, unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "metric_explainer_rf = MetricTextExplainer(metric_test_rf)\n",
    "\n",
    "print(\"RF Classification accuracy = \", metric_test_rf.accuracy())\n",
    "print(\"Equal Opportunity Difference (Test):\", metric_test_rf.equal_opportunity_difference())\n",
    "print(\"Average Odds Difference (Test):\", metric_test_rf.average_odds_difference())\n",
    "print(\"Theil Index (Test):\", metric_test_rf.theil_index())\n",
    "print(\"Statistical Parity Difference/Mean Difference (Test):\", metric_test_rf.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_test_rf.disparate_impact())\n",
    "print(\"Consistency:\", metric_test_rf.consistency())\n",
    "print(\"Error Rate Difference:\", metric_test_rf.error_rate_difference())\n",
    "print(\"False Negative Rate Difference:\", metric_test_rf.false_negative_rate_difference())\n",
    "print(\"False Positive Rate Difference:\", metric_test_rf.false_positive_rate_difference())\n",
    "print(\"Mean Difference\", metric_test_rf.mean_difference())\n",
    "print(\"Binary Confusion Matrix:\", metric_test_rf.binary_confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bias Mitigation: Preprocessing algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 AIF360 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply reweighing to the training data\n",
    "RW = Reweighing(unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "train_dataset_transf_rw = RW.fit_transform(train_dataset)\n",
    "\n",
    "# Ensure train_dataset_transf.features has column names matching X_test if applicable\n",
    "train_dataset_transf_rw.features = pd.DataFrame(train_dataset_transf_rw.features, columns=X_test.columns)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rf_transf_rw = RandomForestClassifier(random_state=42)\n",
    "rf_transf_rw.fit(train_dataset_transf_rw.features, train_dataset_transf_rw.labels.ravel(), train_dataset_transf_rw.instance_weights)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_transf_rw = rf_transf_rw.predict(X_test)\n",
    "\n",
    "# Handle the test dataset as needed\n",
    "test_dataset_pred_transf_rw = test_dataset.copy(deepcopy=True)\n",
    "test_dataset_pred_transf_rw.labels = y_pred_transf_rw.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reweighed RF Classification accuracy =  0.843501326259947\n",
      "Equal Opportunity Difference (Test): -0.5492957746478873\n",
      "Average Odds Difference (Test): -0.2414252959618174\n",
      "Theil Index (Test): 0.12915881340037702\n",
      "Statistical Parity Difference/Mean Difference (Test): -0.060215053763440884\n",
      "Disparate Impact: 0.6989247311827956\n",
      "Consistency: [0.79098143]\n",
      "Error Rate Difference: 0.15860215053763438\n",
      "False Negative Rate Difference: 0.5492957746478874\n",
      "False Positive Rate Difference: 0.0664451827242525\n",
      "Mean Difference -0.060215053763440884\n",
      "Binary Confusion Matrix: {'TP': 33.0, 'FP': 20.0, 'TN': 285.0, 'FN': 39.0}\n"
     ]
    }
   ],
   "source": [
    "# Compute fairness metrics\n",
    "metric_transf_rf_rw = ClassificationMetric(test_dataset, test_dataset_pred_transf_rw, unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "metric_transf_explainer_rf_rw = MetricTextExplainer(metric_transf_rf_rw)\n",
    "\n",
    "print(\"Reweighed RF Classification accuracy = \", metric_transf_rf_rw.accuracy())\n",
    "print(\"Equal Opportunity Difference (Test):\", metric_transf_rf_rw.equal_opportunity_difference())\n",
    "print(\"Average Odds Difference (Test):\", metric_transf_rf_rw.average_odds_difference())\n",
    "print(\"Theil Index (Test):\", metric_transf_rf_rw.theil_index())\n",
    "print(\"Statistical Parity Difference/Mean Difference (Test):\", metric_transf_rf_rw.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_transf_rf_rw.disparate_impact())\n",
    "print(\"Consistency:\", metric_transf_rf_rw.consistency())\n",
    "print(\"Error Rate Difference:\", metric_transf_rf_rw.error_rate_difference())\n",
    "print(\"False Negative Rate Difference:\", metric_transf_rf_rw.false_negative_rate_difference())\n",
    "print(\"False Positive Rate Difference:\", metric_transf_rf_rw.false_positive_rate_difference())\n",
    "print(\"Mean Difference\", metric_transf_rf_rw.mean_difference())\n",
    "print(\"Binary Confusion Matrix:\", metric_transf_rf_rw.binary_confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Learning Fair Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import LFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.6358981263447946, L_x: 0.8690398330499958,  L_y: 0.5330061727454541,  L_z: 0.00799398514717045\n",
      "step: 250, loss: 0.6358981288259229, L_x: 0.8690398373580137,  L_y: 0.5330061793242743,  L_z: 0.007993982882923623\n",
      "step: 500, loss: 0.6103862493372761, L_x: 0.8684063494950421,  L_y: 0.5084576175871551,  L_z: 0.007543998400308396\n",
      "step: 750, loss: 0.5924493964487236, L_x: 0.8647644138461457,  L_y: 0.4931526631889835,  L_z: 0.006410145937562795\n",
      "step: 1000, loss: 0.5924494343213254, L_x: 0.8647644492006197,  L_y: 0.4931527002997446,  L_z: 0.006410144550759361\n",
      "step: 1250, loss: 0.5881701160671977, L_x: 0.8626827648208353,  L_y: 0.4906309223150755,  L_z: 0.005635458635019362\n",
      "step: 1500, loss: 0.5637554003389156, L_x: 0.8400994540792728,  L_y: 0.4713357637025459,  L_z: 0.004204845614221195\n",
      "step: 1750, loss: 0.5637553897977865, L_x: 0.8400994204516657,  L_y: 0.4713357430138533,  L_z: 0.004204852369383326\n",
      "step: 2000, loss: 0.5552673052185193, L_x: 0.8246091126513972,  L_y: 0.46296263477151833,  L_z: 0.004921879590930579\n",
      "step: 2250, loss: 0.5538424307637709, L_x: 0.8221847454900739,  L_y: 0.46193717848549976,  L_z: 0.004843388864631882\n",
      "step: 2500, loss: 0.553842413166208, L_x: 0.8221846900381435,  L_y: 0.461937174300763,  L_z: 0.004843384930815314\n",
      "step: 2750, loss: 0.5497227998920219, L_x: 0.8123665706041698,  L_y: 0.45794009752850406,  L_z: 0.005273022651550451\n",
      "step: 3000, loss: 0.5418711958513253, L_x: 0.7930135331756893,  L_y: 0.45145819968303325,  L_z: 0.005555821425361526\n",
      "step: 3250, loss: 0.5418712121475425, L_x: 0.7930135316954918,  L_y: 0.4514582058668215,  L_z: 0.005555826555585879\n",
      "step: 3500, loss: 0.5274308192983282, L_x: 0.5224368346180593,  L_y: 0.4264998690366894,  L_z: 0.024343633399916458\n",
      "step: 3750, loss: 0.5551421795486655, L_x: 0.5337539625173705,  L_y: 0.4472008052883845,  L_z: 0.027282989004271924\n",
      "step: 4000, loss: 0.5551422021456633, L_x: 0.533753999783907,  L_y: 0.4472008275292206,  L_z: 0.02728298731902601\n",
      "step: 4250, loss: 0.5130729632354897, L_x: 0.5265181174452416,  L_y: 0.4117194511082093,  L_z: 0.02435085019137811\n",
      "step: 4500, loss: 0.5039292376251704, L_x: 0.5347589527856131,  L_y: 0.40768634796371206,  L_z: 0.02138349719144849\n",
      "step: 4750, loss: 0.5039292022492321, L_x: 0.5347590184093002,  L_y: 0.4076863675312211,  L_z: 0.021383466438540465\n",
      "step: 5000, loss: 0.9722999423703956, L_x: 1.2498199117802802,  L_y: 0.7952904661439292,  L_z: 0.02601374252421921\n",
      "step: 5250, loss: 0.4974405655275538, L_x: 0.5484169087553827,  L_y: 0.4048641374062607,  L_z: 0.018867368622877405\n",
      "step: 5500, loss: 0.49744057510142276, L_x: 0.5484168611755698,  L_y: 0.4048641354997201,  L_z: 0.018867376742072846\n"
     ]
    }
   ],
   "source": [
    "# Apply LFR to the training data\n",
    "lfr = LFR(unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}], seed=1, k=10, Ax=0.1, Ay=1.0, Az=2.0,verbose=1)\n",
    "train_dataset_transf_lfr = lfr.fit_transform(train_dataset, maxiter=5000, maxfun=5000)\n",
    "\n",
    "# Ensure train_dataset_transf.features has column names matching X_test if applicable\n",
    "train_dataset_transf_lfr.features = pd.DataFrame(train_dataset_transf_lfr.features, columns=X_test.columns)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rf_transf_lfr = RandomForestClassifier(random_state=42)\n",
    "rf_transf_lfr.fit(train_dataset_transf_lfr.features, train_dataset_transf_lfr.labels.ravel())\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_transf_lfr = rf_transf_lfr.predict(X_test)\n",
    "\n",
    "# Handle the test dataset as needed\n",
    "test_dataset_pred_transf_lfr = test_dataset.copy(deepcopy=True)\n",
    "test_dataset_pred_transf_lfr.labels = y_pred_transf_lfr.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFR RF Classification accuracy =  0.7824933687002652\n",
      "Equal Opportunity Difference (Test): -0.647887323943662\n",
      "Average Odds Difference (Test): -0.3908041270881101\n",
      "Theil Index (Test): 0.162868617323457\n",
      "Statistical Parity Difference/Mean Difference (Test): -0.23870967741935487\n",
      "Disparate Impact: 0.4032258064516129\n",
      "Consistency: [0.79098143]\n",
      "Error Rate Difference: 0.01774193548387104\n",
      "False Negative Rate Difference: 0.647887323943662\n",
      "False Positive Rate Difference: -0.13372093023255816\n",
      "Mean Difference -0.23870967741935487\n",
      "Binary Confusion Matrix: {'TP': 26.0, 'FP': 36.0, 'TN': 269.0, 'FN': 46.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculate fairness metrics\n",
    "metric_transf_rf_lfr = ClassificationMetric(test_dataset, test_dataset_pred_transf_lfr, unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "metric_transf_explainer_rf_lfr = MetricTextExplainer(metric_transf_rf_lfr)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"LFR RF Classification accuracy = \", metric_transf_rf_lfr.accuracy())\n",
    "print(\"Equal Opportunity Difference (Test):\", metric_transf_rf_lfr.equal_opportunity_difference())\n",
    "print(\"Average Odds Difference (Test):\", metric_transf_rf_lfr.average_odds_difference())\n",
    "print(\"Theil Index (Test):\", metric_transf_rf_lfr.theil_index())\n",
    "print(\"Statistical Parity Difference/Mean Difference (Test):\", metric_transf_rf_lfr.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_transf_rf_lfr.disparate_impact())\n",
    "print(\"Consistency:\", metric_transf_rf_lfr.consistency())\n",
    "print(\"Error Rate Difference:\", metric_transf_rf_lfr.error_rate_difference())\n",
    "print(\"False Negative Rate Difference:\", metric_transf_rf_lfr.false_negative_rate_difference())\n",
    "print(\"False Positive Rate Difference:\", metric_transf_rf_lfr.false_positive_rate_difference())\n",
    "print(\"Mean Difference\", metric_transf_rf_lfr.mean_difference())\n",
    "print(\"Binary Confusion Matrix:\", metric_transf_rf_lfr.binary_confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3 Optimized Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We cant make this algorithm work. There is a lack of documentation and examples on how to set this up, because all guides and demo examples provided use built-in datasets (adult, german credit card data). They import the following preproc and distortion data. We found no documentation on what exactly that is and how to apply it to an external dataset. Below is an example of these built-in dataset they use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "            import load_preproc_data_adult\n",
    "# from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions\\\n",
    "            import get_distortion_adult\n",
    "# from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our trial of the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_options = {\n",
    "    \"epsilon\": 0.05,\n",
    "    \"clist\": [0.99, 1.99, 2.99],\n",
    "    \"dlist\": [.1, 0.05, 0]\n",
    "}\n",
    "\n",
    "# Apply OP to the training data\n",
    "OP = OptimPreproc(OptTools, optim_options, seed=1)\n",
    "train_dataset_transf_op = OP.fit_transform(train_dataset)\n",
    "\n",
    "# Ensure train_dataset_transf.features has column names matching X_test if applicable\n",
    "train_dataset_transf_op.features = pd.DataFrame(train_dataset_transf_op.features, columns=X_test.columns)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rf_transf_op = RandomForestClassifier(random_state=42)\n",
    "rf_transf_op.fit(train_dataset_transf_op.features, train_dataset_transf_op.labels.ravel())\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_transf_op = rf_transf_op.predict(X_test)\n",
    "\n",
    "# Handle the test dataset as needed\n",
    "test_dataset_pred_transf_op = test_dataset.copy(deepcopy=True)\n",
    "test_dataset_pred_transf_op.labels = y_pred_transf_op.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we check features and labels\n",
    "print(train_dataset_transf_lfr.features[:10])\n",
    "print(train_dataset_transf_lfr.labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.4 Disparate Impact Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import DisparateImpactRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting BlackBoxAuditing\n",
      "  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from BlackBoxAuditing) (2.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from BlackBoxAuditing) (3.3.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from BlackBoxAuditing) (1.1.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from BlackBoxAuditing) (1.19.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.8/site-packages (from networkx->BlackBoxAuditing) (4.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing) (7.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/conda/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing) (2024.7.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas->BlackBoxAuditing) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib->BlackBoxAuditing) (1.15.0)\n",
      "Building wheels for collected packages: BlackBoxAuditing\n",
      "  Building wheel for BlackBoxAuditing (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394771 sha256=d72e831a8b4f925c039a232eb6881ba127bba074fb4d234d01c7b23fcafa15cc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fwy8956_/wheels/e3/77/36/a32ec1b04c2ebe2c45e88d42f33f22f987e76aad3f297b681e\n",
      "Successfully built BlackBoxAuditing\n",
      "Installing collected packages: BlackBoxAuditing\n",
      "Successfully installed BlackBoxAuditing-0.1.54\n"
     ]
    }
   ],
   "source": [
    "!pip install BlackBoxAuditing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DisparateImpactRemover to the training data\n",
    "DIR = DisparateImpactRemover(repair_level=1.0)\n",
    "train_dataset_transf_dir = DIR.fit_transform(train_dataset)\n",
    "\n",
    "# Ensure train_dataset_transf.features has column names matching X_test if applicable\n",
    "train_dataset_transf_dir.features = pd.DataFrame(train_dataset_transf_dir.features, columns=X_test.columns)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rf_transf_dir = RandomForestClassifier(random_state=42)\n",
    "rf_transf_dir.fit(train_dataset_transf_dir.features, train_dataset_transf_dir.labels.ravel())\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_transf_dir = rf_transf_dir.predict(X_test)\n",
    "\n",
    "# Handle the test dataset as needed\n",
    "test_dataset_pred_transf_dir = test_dataset.copy(deepcopy=True)\n",
    "test_dataset_pred_transf_dir.labels = y_pred_transf_dir.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR RF Classification accuracy =  0.8169761273209549\n",
      "Equal Opportunity Difference (Test): 0.36619718309859156\n",
      "Average Odds Difference (Test): 0.22130457161574096\n",
      "Theil Index (Test): 0.15301824692937033\n",
      "Statistical Parity Difference/Mean Difference (Test): 0.13172043010752688\n",
      "Disparate Impact: inf\n",
      "Consistency: [0.79098143]\n",
      "Error Rate Difference: -0.017204301075268824\n",
      "False Negative Rate Difference: -0.3661971830985915\n",
      "False Positive Rate Difference: 0.07641196013289037\n",
      "Binary Confusion Matrix: {'TP': 26.0, 'FP': 23.0, 'TN': 282.0, 'FN': 46.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    }
   ],
   "source": [
    "# Calculate fairness metrics\n",
    "metric_transf_rf_dir = ClassificationMetric(test_dataset, test_dataset_pred_transf_dir, unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "metric_transf_explainer_rf_dir = MetricTextExplainer(metric_transf_rf_dir)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"DIR RF Classification accuracy = \", metric_transf_rf_dir.accuracy())\n",
    "print(\"Equal Opportunity Difference (Test):\", metric_transf_rf_dir.equal_opportunity_difference())\n",
    "print(\"Average Odds Difference (Test):\", metric_transf_rf_dir.average_odds_difference())\n",
    "print(\"Theil Index (Test):\", metric_transf_rf_dir.theil_index())\n",
    "print(\"Statistical Parity Difference/Mean Difference (Test):\", metric_transf_rf_dir.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_transf_rf_dir.disparate_impact())\n",
    "print(\"Consistency:\", metric_transf_rf_dir.consistency())\n",
    "print(\"Error Rate Difference:\", metric_transf_rf_dir.error_rate_difference())\n",
    "print(\"False Negative Rate Difference:\", metric_transf_rf_dir.false_negative_rate_difference())\n",
    "print(\"False Positive Rate Difference:\", metric_transf_rf_dir.false_positive_rate_difference())\n",
    "print(\"Binary Confusion Matrix:\", metric_transf_rf_dir.binary_confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bias Mitigation: In-processing algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 AIF360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 Prejudice Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.inprocessing import PrejudiceRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PrejudiceRemover to the training data\n",
    "PR = PrejudiceRemover(eta=25.0, sensitive_attr=protected_attribute, class_attr=label_column)\n",
    "train_transf_pr = PR.fit(train_dataset)\n",
    "\n",
    "# Predict on the test data\n",
    "test_pred_pr = PR.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR Classification accuracy =  0.8249336870026526\n",
      "Equal Opportunity Difference (Test): 0.4647887323943662\n",
      "Average Odds Difference (Test): 0.2772448645360535\n",
      "Theil Index (Test): 0.13489563592096332\n",
      "Statistical Parity Difference/Mean Difference (Test): 0.16129032258064516\n",
      "Disparate Impact: inf\n",
      "Consistency: [0.79098143]\n",
      "Error Rate Difference: -0.025268817204301075\n",
      "False Negative Rate Difference: -0.46478873239436624\n",
      "False Positive Rate Difference: 0.08970099667774087\n",
      "Binary Confusion Matrix: {'TP': 33.0, 'FP': 27.0, 'TN': 278.0, 'FN': 39.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    }
   ],
   "source": [
    "# Calculate fairness metrics\n",
    "metric_transf_pr = ClassificationMetric(test_dataset, test_pred_pr, unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "metric_transf_explainer_pr = MetricTextExplainer(metric_transf_pr)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"PR Classification accuracy = \", metric_transf_pr.accuracy())\n",
    "print(\"Equal Opportunity Difference (Test):\", metric_transf_pr.equal_opportunity_difference())\n",
    "print(\"Average Odds Difference (Test):\", metric_transf_pr.average_odds_difference())\n",
    "print(\"Theil Index (Test):\", metric_transf_pr.theil_index())\n",
    "print(\"Statistical Parity Difference/Mean Difference (Test):\", metric_transf_pr.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_transf_pr.disparate_impact())\n",
    "print(\"Consistency:\", metric_transf_pr.consistency())\n",
    "print(\"Error Rate Difference:\", metric_transf_pr.error_rate_difference())\n",
    "print(\"False Negative Rate Difference:\", metric_transf_pr.false_negative_rate_difference())\n",
    "print(\"False Positive Rate Difference:\", metric_transf_pr.false_positive_rate_difference())\n",
    "print(\"Binary Confusion Matrix:\", metric_transf_pr.binary_confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2 Adversial Debiasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Here, many compatibiltiy errors occured. We have to set the dependencies to older versions (tensorflow v1), and additionally restart the kernel after installing everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.3.0\n",
      "  Downloading tensorflow-2.3.0-cp38-cp38-manylinux2010_x86_64.whl (320.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 320.5 MB 1.1 MB/s eta 0:00:012    |█████████████████████████████▍  | 294.8 MB 849 kB/s eta 0:00:31     |█████████████████████████████▋  | 296.3 MB 849 kB/s eta 0:00:29\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.64.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 43.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 37.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.0 MB 71.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.35.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 53.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 29.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 35.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 39.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.6 MB 36.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.12.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.15.0)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 33.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5 MB 38.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from protobuf>=3.9.2->tensorflow==2.3.0) (49.6.0.post20200917)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.32.0-py2.py3-none-any.whl (195 kB)\n",
      "\u001b[K     |████████████████████████████████| 195 kB 40.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 47.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[K     |████████████████████████████████| 227 kB 57.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.24.0)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 45.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2024.7.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 27.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.0)\n",
      "Installing collected packages: grpcio, astunparse, numpy, opt-einsum, scipy, termcolor, absl-py, keras-preprocessing, gast, wrapt, google-pasta, tensorflow-estimator, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, requests-oauthlib, google-auth-oauthlib, importlib-metadata, markdown, tensorboard-data-server, MarkupSafe, werkzeug, tensorboard, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.2\n",
      "    Uninstalling scipy-1.5.2:\n",
      "      Successfully uninstalled scipy-1.5.2\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 2.0.0\n",
      "    Uninstalling importlib-metadata-2.0.0:\n",
      "      Successfully uninstalled importlib-metadata-2.0.0\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 1.1.1\n",
      "    Uninstalling MarkupSafe-1.1.1:\n",
      "      Successfully uninstalled MarkupSafe-1.1.1\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "scikit-learn 1.3.2 requires scipy>=1.5.0, but you'll have scipy 1.4.1 which is incompatible.\n",
      "tensorboard 2.14.0 requires protobuf>=3.19.6, but you'll have protobuf 3.12.4 which is incompatible.\u001b[0m\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 gast-0.3.3 google-auth-2.32.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.64.1 importlib-metadata-8.0.0 keras-preprocessing-1.1.2 markdown-3.6 numpy-1.18.5 opt-einsum-3.3.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 requests-oauthlib-2.0.0 rsa-4.9 scipy-1.4.1 tensorboard-2.14.0 tensorboard-data-server-0.7.2 tensorflow-2.3.0 tensorflow-estimator-2.3.0 termcolor-2.4.0 werkzeug-3.0.3 wrapt-1.16.0\n",
      "Collecting protobuf==3.20.0\n",
      "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.12.4\n",
      "    Uninstalling protobuf-3.12.4:\n",
      "      Successfully uninstalled protobuf-3.12.4\n",
      "Successfully installed protobuf-3.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.3.0\n",
    "!pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aif360[AdversarialDebiasing] in /opt/conda/lib/python3.8/site-packages (0.6.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.8/site-packages (from aif360[AdversarialDebiasing]) (1.1.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from aif360[AdversarialDebiasing]) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /opt/conda/lib/python3.8/site-packages (from aif360[AdversarialDebiasing]) (1.3.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from aif360[AdversarialDebiasing]) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.8/site-packages (from aif360[AdversarialDebiasing]) (1.18.5)\n",
      "Requirement already satisfied: tensorflow>=1.13.1; extra == \"adversarialdebiasing\" in /opt/conda/lib/python3.8/site-packages (from aif360[AdversarialDebiasing]) (2.3.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24.0->aif360[AdversarialDebiasing]) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24.0->aif360[AdversarialDebiasing]) (2.8.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=1.0->aif360[AdversarialDebiasing]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=1.0->aif360[AdversarialDebiasing]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/conda/lib/python3.8/site-packages (from matplotlib->aif360[AdversarialDebiasing]) (2024.7.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib->aif360[AdversarialDebiasing]) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->aif360[AdversarialDebiasing]) (7.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->aif360[AdversarialDebiasing]) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->aif360[AdversarialDebiasing]) (0.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (2.14.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (0.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (2.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.64.1)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.1.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (0.35.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.20.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (0.2.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (2.1.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (2.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (2.24.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (2.32.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.0.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (49.6.0.post20200917)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (0.7.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (5.3.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (8.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'aif360[AdversarialDebiasing]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.inprocessing import AdversarialDebiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.871475; batch adversarial loss: 0.474724\n",
      "epoch 1; iter: 0; batch classifier loss: 0.726922; batch adversarial loss: 0.474183\n",
      "epoch 2; iter: 0; batch classifier loss: 0.674182; batch adversarial loss: 0.504397\n",
      "epoch 3; iter: 0; batch classifier loss: 0.588273; batch adversarial loss: 0.516418\n",
      "epoch 4; iter: 0; batch classifier loss: 0.535421; batch adversarial loss: 0.529674\n",
      "epoch 5; iter: 0; batch classifier loss: 0.496321; batch adversarial loss: 0.541513\n",
      "epoch 6; iter: 0; batch classifier loss: 0.465318; batch adversarial loss: 0.542931\n",
      "epoch 7; iter: 0; batch classifier loss: 0.451534; batch adversarial loss: 0.544777\n",
      "epoch 8; iter: 0; batch classifier loss: 0.396203; batch adversarial loss: 0.551249\n",
      "epoch 9; iter: 0; batch classifier loss: 0.363639; batch adversarial loss: 0.569129\n",
      "epoch 10; iter: 0; batch classifier loss: 0.309709; batch adversarial loss: 0.559047\n",
      "epoch 11; iter: 0; batch classifier loss: 0.385183; batch adversarial loss: 0.552781\n",
      "epoch 12; iter: 0; batch classifier loss: 0.334701; batch adversarial loss: 0.562396\n",
      "epoch 13; iter: 0; batch classifier loss: 0.351651; batch adversarial loss: 0.563822\n",
      "epoch 14; iter: 0; batch classifier loss: 0.332246; batch adversarial loss: 0.575178\n",
      "epoch 15; iter: 0; batch classifier loss: 0.272570; batch adversarial loss: 0.572372\n",
      "epoch 16; iter: 0; batch classifier loss: 0.322975; batch adversarial loss: 0.575491\n",
      "epoch 17; iter: 0; batch classifier loss: 0.348391; batch adversarial loss: 0.564269\n",
      "epoch 18; iter: 0; batch classifier loss: 0.378064; batch adversarial loss: 0.571672\n",
      "epoch 19; iter: 0; batch classifier loss: 0.316504; batch adversarial loss: 0.573179\n",
      "epoch 20; iter: 0; batch classifier loss: 0.375775; batch adversarial loss: 0.571118\n",
      "epoch 21; iter: 0; batch classifier loss: 0.473652; batch adversarial loss: 0.566556\n",
      "epoch 22; iter: 0; batch classifier loss: 0.512008; batch adversarial loss: 0.565676\n",
      "epoch 23; iter: 0; batch classifier loss: 0.384773; batch adversarial loss: 0.560903\n",
      "epoch 24; iter: 0; batch classifier loss: 0.416533; batch adversarial loss: 0.563320\n",
      "epoch 25; iter: 0; batch classifier loss: 0.455000; batch adversarial loss: 0.555102\n",
      "epoch 26; iter: 0; batch classifier loss: 0.350197; batch adversarial loss: 0.546739\n",
      "epoch 27; iter: 0; batch classifier loss: 0.413730; batch adversarial loss: 0.539373\n",
      "epoch 28; iter: 0; batch classifier loss: 0.338110; batch adversarial loss: 0.548880\n",
      "epoch 29; iter: 0; batch classifier loss: 0.388566; batch adversarial loss: 0.534837\n",
      "epoch 30; iter: 0; batch classifier loss: 0.401101; batch adversarial loss: 0.529812\n",
      "epoch 31; iter: 0; batch classifier loss: 0.441098; batch adversarial loss: 0.521100\n",
      "epoch 32; iter: 0; batch classifier loss: 0.429153; batch adversarial loss: 0.530285\n",
      "epoch 33; iter: 0; batch classifier loss: 0.424573; batch adversarial loss: 0.516320\n",
      "epoch 34; iter: 0; batch classifier loss: 0.420733; batch adversarial loss: 0.516212\n",
      "epoch 35; iter: 0; batch classifier loss: 0.383511; batch adversarial loss: 0.518626\n",
      "epoch 36; iter: 0; batch classifier loss: 0.401145; batch adversarial loss: 0.509523\n",
      "epoch 37; iter: 0; batch classifier loss: 0.377276; batch adversarial loss: 0.498724\n",
      "epoch 38; iter: 0; batch classifier loss: 0.384479; batch adversarial loss: 0.503924\n",
      "epoch 39; iter: 0; batch classifier loss: 0.374126; batch adversarial loss: 0.493746\n",
      "epoch 40; iter: 0; batch classifier loss: 0.494154; batch adversarial loss: 0.484494\n",
      "epoch 41; iter: 0; batch classifier loss: 0.355377; batch adversarial loss: 0.491885\n",
      "epoch 42; iter: 0; batch classifier loss: 0.333882; batch adversarial loss: 0.483841\n",
      "epoch 43; iter: 0; batch classifier loss: 0.449794; batch adversarial loss: 0.477241\n",
      "epoch 44; iter: 0; batch classifier loss: 0.283151; batch adversarial loss: 0.475142\n",
      "epoch 45; iter: 0; batch classifier loss: 0.350082; batch adversarial loss: 0.475984\n",
      "epoch 46; iter: 0; batch classifier loss: 0.361075; batch adversarial loss: 0.469855\n",
      "epoch 47; iter: 0; batch classifier loss: 0.441832; batch adversarial loss: 0.467269\n",
      "epoch 48; iter: 0; batch classifier loss: 0.354738; batch adversarial loss: 0.453228\n",
      "epoch 49; iter: 0; batch classifier loss: 0.450272; batch adversarial loss: 0.458248\n"
     ]
    }
   ],
   "source": [
    "# Initialize TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Train Adversarial Debiasing model\n",
    "adv_debias_model = AdversarialDebiasing(unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}],\n",
    "                                        scope_name='adv_debiasing',\n",
    "                                        debias=True,\n",
    "                                        sess=sess)\n",
    "\n",
    "train_transf_adv = adv_debias_model.fit(train_dataset)\n",
    "\n",
    "# Predict on the test set\n",
    "test_pred_adv = adv_debias_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV Classification accuracy =  0.8090185676392573\n",
      "Equal Opportunity Difference (Test): -1.0\n",
      "Average Odds Difference (Test): -0.625\n",
      "Theil Index (Test): 0.2099130566722791\n",
      "Statistical Parity Difference/Mean Difference (Test): -0.4\n",
      "Disparate Impact: 0.0\n",
      "Consistency: [0.79098143]\n",
      "Error Rate Difference: -0.009139784946236462\n",
      "False Negative Rate Difference: 1.0\n",
      "False Positive Rate Difference: -0.25\n",
      "Binary Confusion Matrix: {'TP': 1.0, 'FP': 1.0, 'TN': 304.0, 'FN': 71.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculate fairness metrics\n",
    "metric_transf_adv = ClassificationMetric(test_dataset, test_pred_adv, unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "metric_transf_explainer_adv = MetricTextExplainer(metric_transf_adv)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"ADV Classification accuracy = \", metric_transf_adv.accuracy())\n",
    "print(\"Equal Opportunity Difference (Test):\", metric_transf_adv.equal_opportunity_difference())\n",
    "print(\"Average Odds Difference (Test):\", metric_transf_adv.average_odds_difference())\n",
    "print(\"Theil Index (Test):\", metric_transf_adv.theil_index())\n",
    "print(\"Statistical Parity Difference/Mean Difference (Test):\", metric_transf_adv.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_transf_adv.disparate_impact())\n",
    "print(\"Consistency:\", metric_transf_adv.consistency())\n",
    "print(\"Error Rate Difference:\", metric_transf_adv.error_rate_difference())\n",
    "print(\"False Negative Rate Difference:\", metric_transf_adv.false_negative_rate_difference())\n",
    "print(\"False Positive Rate Difference:\", metric_transf_adv.false_positive_rate_difference())\n",
    "print(\"Binary Confusion Matrix:\", metric_transf_adv.binary_confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3 Exponentiated Gradient Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This does nor work due to depency issues, running it will result in the following error we were not able to fix:\n",
    "\n",
    "NameError: name 'red' is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'aif360[Reductions]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.inprocessing.exponentiated_gradient_reduction import ExponentiatedGradientReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Exponentiated Gradient Reduction model\n",
    "exp_grad_model = ExponentiatedGradientReduction(estimator=LogisticRegression(solver='liblinear'),\n",
    "                                                constraints=\"EqualizedOdds\",\n",
    "                                                drop_prot_attr=False)\n",
    "\n",
    "train_transf_egr = exp_grad_model.fit(train_dataset)\n",
    "\n",
    "# Predict on the test set\n",
    "test_pred_egr = exp_grad_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.4 Grid Search Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The same problem occurs here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.inprocessing import GridSearchReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Grid Search Reduction model\n",
    "grid_search_model = GridSearchReduction(estimator=LogisticRegression(solver='liblinear'),\n",
    "                                        constraints=\"DemographicParity\",\n",
    "                                        grid_size=50,\n",
    "                                        drop_prot_attr=False)\n",
    "\n",
    "train_transf_gsr = grid_search_model.fit(train_dataset)\n",
    "\n",
    "# Predict on the test set\n",
    "test_pred_gsr = grid_search_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.5 Meta Fair Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.inprocessing import MetaFairClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Meta Fair Classifier model\n",
    "meta_fair_model = MetaFairClassifier(sensitive_attr=protected_attribute, type=\"sr\")  # sr for statistical rate\n",
    "\n",
    "train_transf_mfc = meta_fair_model.fit(train_dataset)\n",
    "\n",
    "# Predict on the test set\n",
    "test_pred_mfc = meta_fair_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Fair Classification accuracy =  0.1909814323607427\n",
      "Equal Opportunity Difference (Test): 0.0\n",
      "Average Odds Difference (Test): 0.0\n",
      "Theil Index (Test): 0.02718588171828264\n",
      "Statistical Parity Difference/Mean Difference (Test): 0.0\n",
      "Disparate Impact: 1.0\n",
      "Consistency: [0.79098143]\n",
      "Error Rate Difference: 0.009139784946236462\n",
      "False Negative Rate Difference: 0.0\n",
      "False Positive Rate Difference: 0.0\n",
      "Binary Confusion Matrix: {'TP': 72.0, 'FP': 305.0, 'TN': 0.0, 'FN': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculate fairness metrics\n",
    "metric_transf_mfc = ClassificationMetric(test_dataset, test_pred_mfc, unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "metric_transf_explainer_mfc = MetricTextExplainer(metric_transf_mfc)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Meta Fair Classification accuracy = \", metric_transf_mfc.accuracy())\n",
    "print(\"Equal Opportunity Difference (Test):\", metric_transf_mfc.equal_opportunity_difference())\n",
    "print(\"Average Odds Difference (Test):\", metric_transf_mfc.average_odds_difference())\n",
    "print(\"Theil Index (Test):\", metric_transf_mfc.theil_index())\n",
    "print(\"Statistical Parity Difference/Mean Difference (Test):\", metric_transf_mfc.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_transf_mfc.disparate_impact())\n",
    "print(\"Consistency:\", metric_transf_mfc.consistency())\n",
    "print(\"Error Rate Difference:\", metric_transf_mfc.error_rate_difference())\n",
    "print(\"False Negative Rate Difference:\", metric_transf_mfc.false_negative_rate_difference())\n",
    "print(\"False Positive Rate Difference:\", metric_transf_mfc.false_positive_rate_difference())\n",
    "print(\"Binary Confusion Matrix:\", metric_transf_mfc.binary_confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.6 Gerry Fair Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.inprocessing import GerryFairClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, error: 0.15782493368700265, fairness violation: 0.001037512955188225, violated group size: 0.78315649867374\n",
      "iteration: 2, error: 0.15782493368700265, fairness violation: 0.001037512955188225, violated group size: 0.78315649867374\n",
      "iteration: 3, error: 0.15782493368700265, fairness violation: 0.001037512955188225, violated group size: 0.78315649867374\n",
      "iteration: 4, error: 0.15782493368700265, fairness violation: 0.001037512955188225, violated group size: 0.78315649867374\n",
      "iteration: 5, error: 0.15782493368700265, fairness violation: 0.001037512955188225, violated group size: 0.78315649867374\n"
     ]
    }
   ],
   "source": [
    "# Train GerryFair Classifier model\n",
    "gerry_fair_model = GerryFairClassifier(C=10, printflag=True) # C: Fairness constraint parameter\n",
    "train_transf_gfc = gerry_fair_model.fit(train_dataset)\n",
    "\n",
    "# Predict on the test set\n",
    "test_pred_gfc = gerry_fair_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerry Fair Classification accuracy =  0.8169761273209549\n",
      "Equal Opportunity Difference (Test): 0.43661971830985913\n",
      "Average Odds Difference (Test): 0.26482148706190634\n",
      "Theil Index (Test): 0.1417293475897231\n",
      "Statistical Parity Difference/Mean Difference (Test): 0.1586021505376344\n",
      "Disparate Impact: inf\n",
      "Consistency: [0.79098143]\n",
      "Error Rate Difference: -0.017204301075268824\n",
      "False Negative Rate Difference: -0.43661971830985913\n",
      "False Positive Rate Difference: 0.09302325581395349\n",
      "Binary Confusion Matrix: {'TP': 31.0, 'FP': 28.0, 'TN': 277.0, 'FN': 41.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    }
   ],
   "source": [
    "# Calculate fairness metrics\n",
    "metric_transf_gfc = ClassificationMetric(test_dataset, test_pred_gfc, unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "metric_transf_explainer_gfc = MetricTextExplainer(metric_transf_gfc)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Gerry Fair Classification accuracy = \", metric_transf_gfc.accuracy())\n",
    "print(\"Equal Opportunity Difference (Test):\", metric_transf_gfc.equal_opportunity_difference())\n",
    "print(\"Average Odds Difference (Test):\", metric_transf_gfc.average_odds_difference())\n",
    "print(\"Theil Index (Test):\", metric_transf_gfc.theil_index())\n",
    "print(\"Statistical Parity Difference/Mean Difference (Test):\", metric_transf_gfc.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_transf_gfc.disparate_impact())\n",
    "print(\"Consistency:\", metric_transf_gfc.consistency())\n",
    "print(\"Error Rate Difference:\", metric_transf_gfc.error_rate_difference())\n",
    "print(\"False Negative Rate Difference:\", metric_transf_gfc.false_negative_rate_difference())\n",
    "print(\"False Positive Rate Difference:\", metric_transf_gfc.false_positive_rate_difference())\n",
    "print(\"Binary Confusion Matrix:\", metric_transf_gfc.binary_confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bias mitigation: Postprocessing algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 AIF360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.1 Reject Option Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.postprocessing import RejectOptionClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X_test with y_test to create a DataFrame with both features and labels\n",
    "X_test_with_labels = X_test.copy()\n",
    "X_test_with_labels[label_column] = y_test.values\n",
    "\n",
    "# Convert test data to AIF360 format\n",
    "test_data_aif360 = BinaryLabelDataset(df=X_test_with_labels, label_names=[label_column], protected_attribute_names=[protected_attribute])\n",
    "\n",
    "# Create a copy of the test data for predicted labels\n",
    "test_data_pred_aif360 = test_data_aif360.copy(deepcopy=True)\n",
    "test_data_pred_aif360.labels = y_pred_lr.reshape(-1, 1)\n",
    "\n",
    "# Set up the reject option classification\n",
    "roc = RejectOptionClassification(unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}], low_class_thresh=0.01, high_class_thresh=0.99, num_class_thresh=100, num_ROC_margin=50)\n",
    "\n",
    "# Fit and transform the data using reject option classification\n",
    "roc = roc.fit(test_data_aif360, test_data_pred_aif360)\n",
    "test_data_transf_pred_roc = roc.predict(test_data_pred_aif360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Classification accuracy =  1.0\n",
      "Equal Opportunity Difference (Test): 0.0\n",
      "Average Odds Difference (Test): 0.0\n",
      "Theil Index (Test): 0.0\n",
      "Statistical Parity Difference/Mean Difference (Test): -0.009139784946236573\n",
      "Disparate Impact: 0.9543010752688171\n",
      "Consistency: [0.79098143]\n",
      "Error Rate Difference: 0.0\n",
      "False Negative Rate Difference: 0.0\n",
      "False Positive Rate Difference: 0.0\n",
      "Binary Confusion Matrix: {'TP': 72.0, 'FP': 0.0, 'TN': 305.0, 'FN': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculate fairness metrics\n",
    "metric_transf_roc = ClassificationMetric(test_data_aif360, test_data_transf_pred_roc, unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "metric_transf_explainer_roc = MetricTextExplainer(metric_transf_roc)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"ROC Classification accuracy = \", metric_transf_roc.accuracy())\n",
    "print(\"Equal Opportunity Difference (Test):\", metric_transf_roc.equal_opportunity_difference())\n",
    "print(\"Average Odds Difference (Test):\", metric_transf_roc.average_odds_difference())\n",
    "print(\"Theil Index (Test):\", metric_transf_roc.theil_index())\n",
    "print(\"Statistical Parity Difference/Mean Difference (Test):\", metric_transf_roc.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_transf_roc.disparate_impact())\n",
    "print(\"Consistency:\", metric_transf_roc.consistency())\n",
    "print(\"Error Rate Difference:\", metric_transf_roc.error_rate_difference())\n",
    "print(\"False Negative Rate Difference:\", metric_transf_roc.false_negative_rate_difference())\n",
    "print(\"False Positive Rate Difference:\", metric_transf_roc.false_positive_rate_difference())\n",
    "print(\"Binary Confusion Matrix:\", metric_transf_roc.binary_confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.2 Equalized Odds Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Equalized Odds postprocessing\n",
    "eq_odds = EqOddsPostprocessing(unprivileged_groups=[{protected_attribute: 0}],\n",
    "                               privileged_groups=[{protected_attribute: 1}])\n",
    "\n",
    "# Learn parameters for postprocessing\n",
    "eq_odds = eq_odds.fit(test_data_aif360, test_data_pred_aif360)\n",
    "test_data_transf_pred_eqo = eq_odds.predict(test_data_pred_aif360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQO Classification accuracy =  0.5145888594164456\n",
      "Equal Opportunity Difference (Test): -0.46478873239436624\n",
      "Average Odds Difference (Test): -0.10988606054934258\n",
      "Theil Index (Test): 0.1506492535960875\n",
      "Statistical Parity Difference/Mean Difference (Test): 0.10268817204301073\n",
      "Disparate Impact: 1.2567204301075268\n",
      "Consistency: [0.79098143]\n",
      "Error Rate Difference: 0.28924731182795704\n",
      "False Negative Rate Difference: 0.4647887323943662\n",
      "False Positive Rate Difference: 0.24501661129568109\n",
      "Binary Confusion Matrix: {'TP': 39.0, 'FP': 150.0, 'TN': 155.0, 'FN': 33.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculate fairness metrics\n",
    "metric_transf_eqo = ClassificationMetric(test_data_aif360, test_data_transf_pred_eqo, unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "metric_transf_explainer_eqo = MetricTextExplainer(metric_transf_eqo)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"EQO Classification accuracy = \", metric_transf_eqo.accuracy())\n",
    "print(\"Equal Opportunity Difference (Test):\", metric_transf_eqo.equal_opportunity_difference())\n",
    "print(\"Average Odds Difference (Test):\", metric_transf_eqo.average_odds_difference())\n",
    "print(\"Theil Index (Test):\", metric_transf_eqo.theil_index())\n",
    "print(\"Statistical Parity Difference/Mean Difference (Test):\", metric_transf_eqo.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_transf_eqo.disparate_impact())\n",
    "print(\"Consistency:\", metric_transf_eqo.consistency())\n",
    "print(\"Error Rate Difference:\", metric_transf_eqo.error_rate_difference())\n",
    "print(\"False Negative Rate Difference:\", metric_transf_eqo.false_negative_rate_difference())\n",
    "print(\"False Positive Rate Difference:\", metric_transf_eqo.false_positive_rate_difference())\n",
    "print(\"Binary Confusion Matrix:\", metric_transf_eqo.binary_confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.3 Calibrated Equalized Odds Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Calibrated Equalized Odds postprocessing\n",
    "ceqo = CalibratedEqOddsPostprocessing(unprivileged_groups=[{protected_attribute: 0}],\n",
    "                               privileged_groups=[{protected_attribute: 1}])\n",
    "\n",
    "# Learn parameters for postprocessing\n",
    "ceqo = ceqo.fit(test_data_aif360, test_data_pred_aif360)\n",
    "test_data_transf_pred_ceqo = ceqo.predict(test_data_pred_aif360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEQO Classification accuracy =  1.0\n",
      "Equal Opportunity Difference (Test): 0.0\n",
      "Average Odds Difference (Test): 0.0\n",
      "Theil Index (Test): 0.0\n",
      "Statistical Parity Difference/Mean Difference (Test): -0.009139784946236573\n",
      "Disparate Impact: 0.9543010752688171\n",
      "Consistency: [0.79098143]\n",
      "Error Rate Difference: 0.0\n",
      "False Negative Rate Difference: 0.0\n",
      "False Positive Rate Difference: 0.0\n",
      "Binary Confusion Matrix: {'TP': 72.0, 'FP': 0.0, 'TN': 305.0, 'FN': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculate fairness metrics\n",
    "metric_transf_ceqo = ClassificationMetric(test_data_aif360, test_data_transf_pred_ceqo, unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "metric_transf_explainer_ceqo = MetricTextExplainer(metric_transf_ceqo)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"CEQO Classification accuracy = \", metric_transf_ceqo.accuracy())\n",
    "print(\"Equal Opportunity Difference (Test):\", metric_transf_ceqo.equal_opportunity_difference())\n",
    "print(\"Average Odds Difference (Test):\", metric_transf_ceqo.average_odds_difference())\n",
    "print(\"Theil Index (Test):\", metric_transf_ceqo.theil_index())\n",
    "print(\"Statistical Parity Difference/Mean Difference (Test):\", metric_transf_ceqo.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_transf_ceqo.disparate_impact())\n",
    "print(\"Consistency:\", metric_transf_ceqo.consistency())\n",
    "print(\"Error Rate Difference:\", metric_transf_ceqo.error_rate_difference())\n",
    "print(\"False Negative Rate Difference:\", metric_transf_ceqo.false_negative_rate_difference())\n",
    "print(\"False Positive Rate Difference:\", metric_transf_ceqo.false_positive_rate_difference())\n",
    "print(\"Binary Confusion Matrix:\", metric_transf_ceqo.binary_confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.4 Deterministic Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is not applicable to our use case, as it is used for fair ranked candidate lists. Example Iplementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.postprocessing import DeterministicReranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Deterministic Reranking postprocessing\n",
    "dr = DeterministicReranking(unprivileged_groups=[{protected_attribute: 0}],\n",
    "                               privileged_groups=[{protected_attribute: 1}])\n",
    "\n",
    "# Learn parameters for postprocessing\n",
    "dr = dr.fit(test_data_aif360, test_data_pred_aif360)\n",
    "test_data_transf_pred_dr = dr.predict(test_data_pred_aif360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Metric Summary Table & Interpretation for AIF360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold all metrics\n",
    "metrics_summary = {\n",
    "    \"Metric\": [\n",
    "        \"Statistical Parity Difference\", \"Disparate Impact\", \"Consistency\",\n",
    "        \"Classification Accuracy\", \"Equal Opportunity Difference\", \"Average Odds Difference\",\n",
    "        \"Theil Index\", \"Error Rate Difference\", \"False Negative Rate Difference\",\n",
    "        \"False Positive Rate Difference\", \"Binary Confusion Matrix\"\n",
    "    ],\n",
    "    \"Original Data\": [\n",
    "        metrics_for_original_data.statistical_parity_difference(),\n",
    "        metrics_for_original_data.disparate_impact(),\n",
    "        metrics_for_original_data.consistency(),\n",
    "        None, None, None, None, None, None, None, None\n",
    "    ],\n",
    "    \"LR\": [\n",
    "        metric_test_lr.statistical_parity_difference(),\n",
    "        metric_test_lr.disparate_impact(),\n",
    "        metric_test_lr.consistency(),\n",
    "        metric_test_lr.accuracy(),\n",
    "        metric_test_lr.equal_opportunity_difference(),\n",
    "        metric_test_lr.average_odds_difference(),\n",
    "        metric_test_lr.theil_index(),\n",
    "        metric_test_lr.error_rate_difference(),\n",
    "        metric_test_lr.false_negative_rate_difference(),\n",
    "        metric_test_lr.false_positive_rate_difference(),\n",
    "        metric_test_lr.binary_confusion_matrix()\n",
    "    ],\n",
    "    \"RF\": [\n",
    "        metric_test_rf.statistical_parity_difference(),\n",
    "        metric_test_rf.disparate_impact(),\n",
    "        metric_test_rf.consistency(),\n",
    "        metric_test_rf.accuracy(),\n",
    "        metric_test_rf.equal_opportunity_difference(),\n",
    "        metric_test_rf.average_odds_difference(),\n",
    "        metric_test_rf.theil_index(),\n",
    "        metric_test_rf.error_rate_difference(),\n",
    "        metric_test_rf.false_negative_rate_difference(),\n",
    "        metric_test_rf.false_positive_rate_difference(),\n",
    "        metric_test_rf.binary_confusion_matrix()\n",
    "    ],\n",
    "    \"Preprocessing RW\": [\n",
    "        metric_transf_rf_rw.statistical_parity_difference(),\n",
    "        metric_transf_rf_rw.disparate_impact(),\n",
    "        metric_transf_rf_rw.consistency(),\n",
    "        metric_transf_rf_rw.accuracy(),\n",
    "        metric_transf_rf_rw.equal_opportunity_difference(),\n",
    "        metric_transf_rf_rw.average_odds_difference(),\n",
    "        metric_transf_rf_rw.theil_index(),\n",
    "        metric_transf_rf_rw.error_rate_difference(),\n",
    "        metric_transf_rf_rw.false_negative_rate_difference(),\n",
    "        metric_transf_rf_rw.false_positive_rate_difference(),\n",
    "        metric_transf_rf_rw.binary_confusion_matrix()\n",
    "    ],\n",
    "    \"Preprocessing LFR\": [\n",
    "        metric_transf_rf_lfr.statistical_parity_difference(),\n",
    "        metric_transf_rf_lfr.disparate_impact(),\n",
    "        metric_transf_rf_lfr.consistency(),\n",
    "        metric_transf_rf_lfr.accuracy(),\n",
    "        metric_transf_rf_lfr.equal_opportunity_difference(),\n",
    "        metric_transf_rf_lfr.average_odds_difference(),\n",
    "        metric_transf_rf_lfr.theil_index(),\n",
    "        metric_transf_rf_lfr.error_rate_difference(),\n",
    "        metric_transf_rf_lfr.false_negative_rate_difference(),\n",
    "        metric_transf_rf_lfr.false_positive_rate_difference(),\n",
    "        metric_transf_rf_lfr.binary_confusion_matrix()\n",
    "    ],\n",
    "    \"Preprocessing DIR\": [\n",
    "        metric_transf_rf_dir.statistical_parity_difference(),\n",
    "        metric_transf_rf_dir.disparate_impact(),\n",
    "        metric_transf_rf_dir.consistency(),\n",
    "        metric_transf_rf_dir.accuracy(),\n",
    "        metric_transf_rf_dir.equal_opportunity_difference(),\n",
    "        metric_transf_rf_dir.average_odds_difference(),\n",
    "        metric_transf_rf_dir.theil_index(),\n",
    "        metric_transf_rf_dir.error_rate_difference(),\n",
    "        metric_transf_rf_dir.false_negative_rate_difference(),\n",
    "        metric_transf_rf_dir.false_positive_rate_difference(),\n",
    "        metric_transf_rf_dir.binary_confusion_matrix()\n",
    "    ],\n",
    "    \"Inprocessing PR\": [\n",
    "        metric_transf_pr.statistical_parity_difference(),\n",
    "        metric_transf_pr.disparate_impact(),\n",
    "        metric_transf_pr.consistency(),\n",
    "        metric_transf_pr.accuracy(),\n",
    "        metric_transf_pr.equal_opportunity_difference(),\n",
    "        metric_transf_pr.average_odds_difference(),\n",
    "        metric_transf_pr.theil_index(),\n",
    "        metric_transf_pr.error_rate_difference(),\n",
    "        metric_transf_pr.false_negative_rate_difference(),\n",
    "        metric_transf_pr.false_positive_rate_difference(),\n",
    "        metric_transf_pr.binary_confusion_matrix()\n",
    "    ],\n",
    "    \"Inprocessing ADV\": [\n",
    "        metric_transf_adv.statistical_parity_difference(),\n",
    "        metric_transf_adv.disparate_impact(),\n",
    "        metric_transf_adv.consistency(),\n",
    "        metric_transf_adv.accuracy(),\n",
    "        metric_transf_adv.equal_opportunity_difference(),\n",
    "        metric_transf_adv.average_odds_difference(),\n",
    "        metric_transf_adv.theil_index(),\n",
    "        metric_transf_adv.error_rate_difference(),\n",
    "        metric_transf_adv.false_negative_rate_difference(),\n",
    "        metric_transf_adv.false_positive_rate_difference(),\n",
    "        metric_transf_adv.binary_confusion_matrix()\n",
    "    ],\n",
    "    \"Inprocessing MF\": [\n",
    "        metric_transf_mfc.statistical_parity_difference(),\n",
    "        metric_transf_mfc.disparate_impact(),\n",
    "        metric_transf_mfc.consistency(),\n",
    "        metric_transf_mfc.accuracy(),\n",
    "        metric_transf_mfc.equal_opportunity_difference(),\n",
    "        metric_transf_mfc.average_odds_difference(),\n",
    "        metric_transf_mfc.theil_index(),\n",
    "        metric_transf_mfc.error_rate_difference(),\n",
    "        metric_transf_mfc.false_negative_rate_difference(),\n",
    "        metric_transf_mfc.false_positive_rate_difference(),\n",
    "        metric_transf_mfc.binary_confusion_matrix()\n",
    "    ],\n",
    "    \"Inprocessing GF\": [\n",
    "        metric_transf_gfc.statistical_parity_difference(),\n",
    "        metric_transf_gfc.disparate_impact(),\n",
    "        metric_transf_gfc.consistency(),\n",
    "        metric_transf_gfc.accuracy(),\n",
    "        metric_transf_gfc.equal_opportunity_difference(),\n",
    "        metric_transf_gfc.average_odds_difference(),\n",
    "        metric_transf_gfc.theil_index(),\n",
    "        metric_transf_gfc.error_rate_difference(),\n",
    "        metric_transf_gfc.false_negative_rate_difference(),\n",
    "        metric_transf_gfc.false_positive_rate_difference(),\n",
    "        metric_transf_gfc.binary_confusion_matrix()\n",
    "    ],\n",
    "    \"Postprocessing ROC\": [\n",
    "        metric_transf_roc.statistical_parity_difference(),\n",
    "        metric_transf_roc.disparate_impact(),\n",
    "        metric_transf_roc.consistency(),\n",
    "        metric_transf_roc.accuracy(),\n",
    "        metric_transf_roc.equal_opportunity_difference(),\n",
    "        metric_transf_roc.average_odds_difference(),\n",
    "        metric_transf_roc.theil_index(),\n",
    "        metric_transf_roc.error_rate_difference(),\n",
    "        metric_transf_roc.false_negative_rate_difference(),\n",
    "        metric_transf_roc.false_positive_rate_difference(),\n",
    "        metric_transf_roc.binary_confusion_matrix()\n",
    "    ],\n",
    "    \"Postprocessing EQO\": [\n",
    "        metric_transf_eqo.statistical_parity_difference(),\n",
    "        metric_transf_eqo.disparate_impact(),\n",
    "        metric_transf_eqo.consistency(),\n",
    "        metric_transf_eqo.accuracy(),\n",
    "        metric_transf_eqo.equal_opportunity_difference(),\n",
    "        metric_transf_eqo.average_odds_difference(),\n",
    "        metric_transf_eqo.theil_index(),\n",
    "        metric_transf_eqo.error_rate_difference(),\n",
    "        metric_transf_eqo.false_negative_rate_difference(),\n",
    "        metric_transf_eqo.false_positive_rate_difference(),\n",
    "        metric_transf_eqo.binary_confusion_matrix()\n",
    "    ],\n",
    "    \"Postprocessing CEQO\": [\n",
    "        metric_transf_ceqo.statistical_parity_difference(),\n",
    "        metric_transf_ceqo.disparate_impact(),\n",
    "        metric_transf_ceqo.consistency(),\n",
    "        metric_transf_ceqo.accuracy(),\n",
    "        metric_transf_ceqo.equal_opportunity_difference(),\n",
    "        metric_transf_ceqo.average_odds_difference(),\n",
    "        metric_transf_ceqo.theil_index(),\n",
    "        metric_transf_ceqo.error_rate_difference(),\n",
    "        metric_transf_ceqo.false_negative_rate_difference(),\n",
    "        metric_transf_ceqo.false_positive_rate_difference(),\n",
    "        metric_transf_ceqo.binary_confusion_matrix()\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "metrics_df = pd.DataFrame(metrics_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Original Data</th>\n",
       "      <th>LR</th>\n",
       "      <th>RF</th>\n",
       "      <th>Preprocessing RW</th>\n",
       "      <th>Preprocessing LFR</th>\n",
       "      <th>Preprocessing DIR</th>\n",
       "      <th>Inprocessing PR</th>\n",
       "      <th>Inprocessing ADV</th>\n",
       "      <th>Inprocessing MF</th>\n",
       "      <th>Inprocessing GF</th>\n",
       "      <th>Postprocessing ROC</th>\n",
       "      <th>Postprocessing EQO</th>\n",
       "      <th>Postprocessing CEQO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Statistical Parity Difference</td>\n",
       "      <td>0.139178</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.145161</td>\n",
       "      <td>-0.0602151</td>\n",
       "      <td>-0.23871</td>\n",
       "      <td>0.13172</td>\n",
       "      <td>0.16129</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.158602</td>\n",
       "      <td>-0.00913978</td>\n",
       "      <td>0.102688</td>\n",
       "      <td>-0.00913978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Disparate Impact</td>\n",
       "      <td>3.29644</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.698925</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.954301</td>\n",
       "      <td>1.25672</td>\n",
       "      <td>0.954301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consistency</td>\n",
       "      <td>[0.8133687002652531]</td>\n",
       "      <td>[0.7909814323607424]</td>\n",
       "      <td>[0.7909814323607424]</td>\n",
       "      <td>[0.7909814323607424]</td>\n",
       "      <td>[0.7909814323607424]</td>\n",
       "      <td>[0.7909814323607424]</td>\n",
       "      <td>[0.7909814323607424]</td>\n",
       "      <td>[0.7909814323607424]</td>\n",
       "      <td>[0.7909814323607424]</td>\n",
       "      <td>[0.7909814323607424]</td>\n",
       "      <td>[0.7909814323607424]</td>\n",
       "      <td>[0.7909814323607424]</td>\n",
       "      <td>[0.7909814323607424]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Classification Accuracy</td>\n",
       "      <td>None</td>\n",
       "      <td>0.824934</td>\n",
       "      <td>0.824934</td>\n",
       "      <td>0.843501</td>\n",
       "      <td>0.782493</td>\n",
       "      <td>0.816976</td>\n",
       "      <td>0.824934</td>\n",
       "      <td>0.809019</td>\n",
       "      <td>0.190981</td>\n",
       "      <td>0.816976</td>\n",
       "      <td>1</td>\n",
       "      <td>0.514589</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Equal Opportunity Difference</td>\n",
       "      <td>None</td>\n",
       "      <td>0.478873</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>-0.549296</td>\n",
       "      <td>-0.647887</td>\n",
       "      <td>0.366197</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.43662</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.464789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Average Odds Difference</td>\n",
       "      <td>None</td>\n",
       "      <td>0.285948</td>\n",
       "      <td>0.251135</td>\n",
       "      <td>-0.241425</td>\n",
       "      <td>-0.390804</td>\n",
       "      <td>0.221305</td>\n",
       "      <td>0.277245</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264821</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.109886</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Theil Index</td>\n",
       "      <td>None</td>\n",
       "      <td>0.13265</td>\n",
       "      <td>0.1416</td>\n",
       "      <td>0.129159</td>\n",
       "      <td>0.162869</td>\n",
       "      <td>0.153018</td>\n",
       "      <td>0.134896</td>\n",
       "      <td>0.209913</td>\n",
       "      <td>0.0271859</td>\n",
       "      <td>0.141729</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Error Rate Difference</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.0252688</td>\n",
       "      <td>-0.0252688</td>\n",
       "      <td>0.158602</td>\n",
       "      <td>0.0177419</td>\n",
       "      <td>-0.0172043</td>\n",
       "      <td>-0.0252688</td>\n",
       "      <td>-0.00913978</td>\n",
       "      <td>0.00913978</td>\n",
       "      <td>-0.0172043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.289247</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False Negative Rate Difference</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.478873</td>\n",
       "      <td>-0.422535</td>\n",
       "      <td>0.549296</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>-0.366197</td>\n",
       "      <td>-0.464789</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.43662</td>\n",
       "      <td>0</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False Positive Rate Difference</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0930233</td>\n",
       "      <td>0.0797342</td>\n",
       "      <td>0.0664452</td>\n",
       "      <td>-0.133721</td>\n",
       "      <td>0.076412</td>\n",
       "      <td>0.089701</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0930233</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Binary Confusion Matrix</td>\n",
       "      <td>None</td>\n",
       "      <td>{'TP': 34.0, 'FP': 28.0, 'TN': 277.0, 'FN': 38.0}</td>\n",
       "      <td>{'TP': 30.0, 'FP': 24.0, 'TN': 281.0, 'FN': 42.0}</td>\n",
       "      <td>{'TP': 33.0, 'FP': 20.0, 'TN': 285.0, 'FN': 39.0}</td>\n",
       "      <td>{'TP': 26.0, 'FP': 36.0, 'TN': 269.0, 'FN': 46.0}</td>\n",
       "      <td>{'TP': 26.0, 'FP': 23.0, 'TN': 282.0, 'FN': 46.0}</td>\n",
       "      <td>{'TP': 33.0, 'FP': 27.0, 'TN': 278.0, 'FN': 39.0}</td>\n",
       "      <td>{'TP': 1.0, 'FP': 1.0, 'TN': 304.0, 'FN': 71.0}</td>\n",
       "      <td>{'TP': 72.0, 'FP': 305.0, 'TN': 0.0, 'FN': 0.0}</td>\n",
       "      <td>{'TP': 31.0, 'FP': 28.0, 'TN': 277.0, 'FN': 41.0}</td>\n",
       "      <td>{'TP': 72.0, 'FP': 0.0, 'TN': 305.0, 'FN': 0.0}</td>\n",
       "      <td>{'TP': 39.0, 'FP': 150.0, 'TN': 155.0, 'FN': 3...</td>\n",
       "      <td>{'TP': 72.0, 'FP': 0.0, 'TN': 305.0, 'FN': 0.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Metric         Original Data  \\\n",
       "0    Statistical Parity Difference              0.139178   \n",
       "1                 Disparate Impact               3.29644   \n",
       "2                      Consistency  [0.8133687002652531]   \n",
       "3          Classification Accuracy                  None   \n",
       "4     Equal Opportunity Difference                  None   \n",
       "5          Average Odds Difference                  None   \n",
       "6                      Theil Index                  None   \n",
       "7            Error Rate Difference                  None   \n",
       "8   False Negative Rate Difference                  None   \n",
       "9   False Positive Rate Difference                  None   \n",
       "10         Binary Confusion Matrix                  None   \n",
       "\n",
       "                                                   LR  \\\n",
       "0                                            0.166667   \n",
       "1                                                 inf   \n",
       "2                                [0.7909814323607424]   \n",
       "3                                            0.824934   \n",
       "4                                            0.478873   \n",
       "5                                            0.285948   \n",
       "6                                             0.13265   \n",
       "7                                          -0.0252688   \n",
       "8                                           -0.478873   \n",
       "9                                           0.0930233   \n",
       "10  {'TP': 34.0, 'FP': 28.0, 'TN': 277.0, 'FN': 38.0}   \n",
       "\n",
       "                                                   RF  \\\n",
       "0                                            0.145161   \n",
       "1                                                 inf   \n",
       "2                                [0.7909814323607424]   \n",
       "3                                            0.824934   \n",
       "4                                            0.422535   \n",
       "5                                            0.251135   \n",
       "6                                              0.1416   \n",
       "7                                          -0.0252688   \n",
       "8                                           -0.422535   \n",
       "9                                           0.0797342   \n",
       "10  {'TP': 30.0, 'FP': 24.0, 'TN': 281.0, 'FN': 42.0}   \n",
       "\n",
       "                                     Preprocessing RW  \\\n",
       "0                                          -0.0602151   \n",
       "1                                            0.698925   \n",
       "2                                [0.7909814323607424]   \n",
       "3                                            0.843501   \n",
       "4                                           -0.549296   \n",
       "5                                           -0.241425   \n",
       "6                                            0.129159   \n",
       "7                                            0.158602   \n",
       "8                                            0.549296   \n",
       "9                                           0.0664452   \n",
       "10  {'TP': 33.0, 'FP': 20.0, 'TN': 285.0, 'FN': 39.0}   \n",
       "\n",
       "                                    Preprocessing LFR  \\\n",
       "0                                            -0.23871   \n",
       "1                                            0.403226   \n",
       "2                                [0.7909814323607424]   \n",
       "3                                            0.782493   \n",
       "4                                           -0.647887   \n",
       "5                                           -0.390804   \n",
       "6                                            0.162869   \n",
       "7                                           0.0177419   \n",
       "8                                            0.647887   \n",
       "9                                           -0.133721   \n",
       "10  {'TP': 26.0, 'FP': 36.0, 'TN': 269.0, 'FN': 46.0}   \n",
       "\n",
       "                                    Preprocessing DIR  \\\n",
       "0                                             0.13172   \n",
       "1                                                 inf   \n",
       "2                                [0.7909814323607424]   \n",
       "3                                            0.816976   \n",
       "4                                            0.366197   \n",
       "5                                            0.221305   \n",
       "6                                            0.153018   \n",
       "7                                          -0.0172043   \n",
       "8                                           -0.366197   \n",
       "9                                            0.076412   \n",
       "10  {'TP': 26.0, 'FP': 23.0, 'TN': 282.0, 'FN': 46.0}   \n",
       "\n",
       "                                      Inprocessing PR  \\\n",
       "0                                             0.16129   \n",
       "1                                                 inf   \n",
       "2                                [0.7909814323607424]   \n",
       "3                                            0.824934   \n",
       "4                                            0.464789   \n",
       "5                                            0.277245   \n",
       "6                                            0.134896   \n",
       "7                                          -0.0252688   \n",
       "8                                           -0.464789   \n",
       "9                                            0.089701   \n",
       "10  {'TP': 33.0, 'FP': 27.0, 'TN': 278.0, 'FN': 39.0}   \n",
       "\n",
       "                                   Inprocessing ADV  \\\n",
       "0                                              -0.4   \n",
       "1                                                 0   \n",
       "2                              [0.7909814323607424]   \n",
       "3                                          0.809019   \n",
       "4                                                -1   \n",
       "5                                            -0.625   \n",
       "6                                          0.209913   \n",
       "7                                       -0.00913978   \n",
       "8                                                 1   \n",
       "9                                             -0.25   \n",
       "10  {'TP': 1.0, 'FP': 1.0, 'TN': 304.0, 'FN': 71.0}   \n",
       "\n",
       "                                    Inprocessing MF  \\\n",
       "0                                                 0   \n",
       "1                                                 1   \n",
       "2                              [0.7909814323607424]   \n",
       "3                                          0.190981   \n",
       "4                                                 0   \n",
       "5                                                 0   \n",
       "6                                         0.0271859   \n",
       "7                                        0.00913978   \n",
       "8                                                 0   \n",
       "9                                                 0   \n",
       "10  {'TP': 72.0, 'FP': 305.0, 'TN': 0.0, 'FN': 0.0}   \n",
       "\n",
       "                                      Inprocessing GF  \\\n",
       "0                                            0.158602   \n",
       "1                                                 inf   \n",
       "2                                [0.7909814323607424]   \n",
       "3                                            0.816976   \n",
       "4                                             0.43662   \n",
       "5                                            0.264821   \n",
       "6                                            0.141729   \n",
       "7                                          -0.0172043   \n",
       "8                                            -0.43662   \n",
       "9                                           0.0930233   \n",
       "10  {'TP': 31.0, 'FP': 28.0, 'TN': 277.0, 'FN': 41.0}   \n",
       "\n",
       "                                 Postprocessing ROC  \\\n",
       "0                                       -0.00913978   \n",
       "1                                          0.954301   \n",
       "2                              [0.7909814323607424]   \n",
       "3                                                 1   \n",
       "4                                                 0   \n",
       "5                                                 0   \n",
       "6                                                 0   \n",
       "7                                                 0   \n",
       "8                                                 0   \n",
       "9                                                 0   \n",
       "10  {'TP': 72.0, 'FP': 0.0, 'TN': 305.0, 'FN': 0.0}   \n",
       "\n",
       "                                   Postprocessing EQO  \\\n",
       "0                                            0.102688   \n",
       "1                                             1.25672   \n",
       "2                                [0.7909814323607424]   \n",
       "3                                            0.514589   \n",
       "4                                           -0.464789   \n",
       "5                                           -0.109886   \n",
       "6                                            0.150649   \n",
       "7                                            0.289247   \n",
       "8                                            0.464789   \n",
       "9                                            0.245017   \n",
       "10  {'TP': 39.0, 'FP': 150.0, 'TN': 155.0, 'FN': 3...   \n",
       "\n",
       "                                Postprocessing CEQO  \n",
       "0                                       -0.00913978  \n",
       "1                                          0.954301  \n",
       "2                              [0.7909814323607424]  \n",
       "3                                                 1  \n",
       "4                                                 0  \n",
       "5                                                 0  \n",
       "6                                                 0  \n",
       "7                                                 0  \n",
       "8                                                 0  \n",
       "9                                                 0  \n",
       "10  {'TP': 72.0, 'FP': 0.0, 'TN': 305.0, 'FN': 0.0}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save results to csv\n",
    "metrics_df.to_csv(\"summary_metrics_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
